{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from comet_ml import Experiment\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "def main(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters,\n",
    "         local_epochs, optimizer, numedges, times, commet, gpu):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "    for i in range(times):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            if(dataset == \"human_activity\"):\n",
    "                model = Mclr_CrossEntropy(561,6).to(device), model\n",
    "                print(\"Hey\")\n",
    "            else:\n",
    "                model = Mclr_Logistic().to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(40,1).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(300).to(device), model\n",
    "        \n",
    "        if model == \"MLP\" and dataset == \"a9a\":\n",
    "            model = DNN( input_dim = 123, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"human_activity\":\n",
    "            model = DNN( input_dim = 561, mid_dim = 561, output_dim = 6).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"w8a\":\n",
    "            model = DNN( input_dim = 300, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"Mnist\":\n",
    "            model = DNN().to(device), model\n",
    "        if model == 'CNN':\n",
    "            model = Net().to(device), model\n",
    "        # select algorithm\n",
    "        if(commet):\n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\")\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNewton_params = {\\n    \"dataset\": \"Mnist\",\\n    \"algorithm\": \"Newton\",\\n    \"model\": \"MLP\",\\n    \"batch_size\": 0,\\n    \"learning_rate\": 1,\\n    \"alpha\": 0.03,\\n    \"eta\": 1.0,\\n    \"L\": 1e-3,\\n    \"rho\": 0.01,\\n    \"num_glob_iters\": 500,\\n    \"local_epochs\": 40,\\n    \"optimizer\": \"Newton\",\\n    \"numedges\": 30,\\n    \"times\": 1,\\n    \"commet\": True,\\n    \"gpu\": 0\\n}\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sophiaOTA_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia_OTA\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 1280,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "sophia_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.003,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 41,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "GD_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"GD\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "FedProx = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"FedProx\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.01,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 10,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "'''\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/abdulmomen96/sophia/6cb134ba49b44524b530029a7e3e5858\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------ 0\n",
      "Number of edges / total edges: 30  /  32\n",
      "-------------Round number:  0  -------------\n",
      "Average Test Accuracy          :  0.10809399477806789\n",
      "Average Global Trainning Accuracy:  0.10628000619482732\n",
      "Average Global Trainning Loss    :  2.3275186752264982\n",
      "-------------Round number:  1  -------------\n",
      "Average Test Accuracy          :  0.27496373658253553\n",
      "Average Global Trainning Accuracy:  0.28141938980950904\n",
      "Average Global Trainning Loss    :  2.205664485974524\n",
      "-------------Round number:  2  -------------\n",
      "Average Test Accuracy          :  0.39094865100087034\n",
      "Average Global Trainning Accuracy:  0.4022765990398018\n",
      "Average Global Trainning Loss    :  2.086571955581152\n",
      "-------------Round number:  3  -------------\n",
      "Average Test Accuracy          :  0.46475195822454307\n",
      "Average Global Trainning Accuracy:  0.48029270559083165\n",
      "Average Global Trainning Loss    :  1.9781166515022457\n",
      "-------------Round number:  4  -------------\n",
      "Average Test Accuracy          :  0.5191760951552075\n",
      "Average Global Trainning Accuracy:  0.5366075576893294\n",
      "Average Global Trainning Loss    :  1.8715647141667957\n",
      "-------------Round number:  5  -------------\n",
      "Average Test Accuracy          :  0.5579924572091673\n",
      "Average Global Trainning Accuracy:  0.581694285271798\n",
      "Average Global Trainning Loss    :  1.7704017926281554\n",
      "-------------Round number:  6  -------------\n",
      "Average Test Accuracy          :  0.5897882216420075\n",
      "Average Global Trainning Accuracy:  0.6161723710701564\n",
      "Average Global Trainning Loss    :  1.6751661532542201\n",
      "-------------Round number:  7  -------------\n",
      "Average Test Accuracy          :  0.6155497534087613\n",
      "Average Global Trainning Accuracy:  0.6436812761344277\n",
      "Average Global Trainning Loss    :  1.584853405219142\n",
      "-------------Round number:  8  -------------\n",
      "Average Test Accuracy          :  0.6378299970989266\n",
      "Average Global Trainning Accuracy:  0.6678411026792628\n",
      "Average Global Trainning Loss    :  1.500897765990398\n",
      "-------------Round number:  9  -------------\n",
      "Average Test Accuracy          :  0.6583695967507978\n",
      "Average Global Trainning Accuracy:  0.6889422332352486\n",
      "Average Global Trainning Loss    :  1.4203757489449436\n",
      "-------------Round number:  10  -------------\n",
      "Average Test Accuracy          :  0.6762402088772846\n",
      "Average Global Trainning Accuracy:  0.7069653089670126\n",
      "Average Global Trainning Loss    :  1.3408334704584173\n",
      "-------------Round number:  11  -------------\n",
      "Average Test Accuracy          :  0.6927183057731361\n",
      "Average Global Trainning Accuracy:  0.7241559547777606\n",
      "Average Global Trainning Loss    :  1.2669188659110269\n",
      "-------------Round number:  12  -------------\n",
      "Average Test Accuracy          :  0.7072236727589208\n",
      "Average Global Trainning Accuracy:  0.7385976459656187\n",
      "Average Global Trainning Loss    :  1.2002985344151695\n",
      "-------------Round number:  13  -------------\n",
      "Average Test Accuracy          :  0.7199303742384683\n",
      "Average Global Trainning Accuracy:  0.752710236952145\n",
      "Average Global Trainning Loss    :  1.1368355835962909\n",
      "-------------Round number:  14  -------------\n",
      "Average Test Accuracy          :  0.7319988395706412\n",
      "Average Global Trainning Accuracy:  0.7654289917918538\n",
      "Average Global Trainning Loss    :  1.078151618398637\n",
      "-------------Round number:  15  -------------\n",
      "Average Test Accuracy          :  0.7430229184798376\n",
      "Average Global Trainning Accuracy:  0.777702493417996\n",
      "Average Global Trainning Loss    :  1.0233850194072325\n",
      "-------------Round number:  16  -------------\n",
      "Average Test Accuracy          :  0.7548592979402379\n",
      "Average Global Trainning Accuracy:  0.7887563884156729\n",
      "Average Global Trainning Loss    :  0.9718290378900806\n",
      "-------------Round number:  17  -------------\n",
      "Average Test Accuracy          :  0.7637365825355381\n",
      "Average Global Trainning Accuracy:  0.79839708843116\n",
      "Average Global Trainning Loss    :  0.9249055651811987\n",
      "-------------Round number:  18  -------------\n",
      "Average Test Accuracy          :  0.7728459530026109\n",
      "Average Global Trainning Accuracy:  0.8081152237881369\n",
      "Average Global Trainning Loss    :  0.8806212462008286\n",
      "-------------Round number:  19  -------------\n",
      "Average Test Accuracy          :  0.7811430229184798\n",
      "Average Global Trainning Accuracy:  0.8176397707913892\n",
      "Average Global Trainning Loss    :  0.8394927198679727\n",
      "-------------Round number:  20  -------------\n",
      "Average Test Accuracy          :  0.7899042645778939\n",
      "Average Global Trainning Accuracy:  0.8259253523308038\n",
      "Average Global Trainning Loss    :  0.8001792355921867\n",
      "-------------Round number:  21  -------------\n",
      "Average Test Accuracy          :  0.7947200464171743\n",
      "Average Global Trainning Accuracy:  0.8338818336688865\n",
      "Average Global Trainning Loss    :  0.7644310287043131\n",
      "-------------Round number:  22  -------------\n",
      "Average Test Accuracy          :  0.8001160429358862\n",
      "Average Global Trainning Accuracy:  0.8400185844819575\n",
      "Average Global Trainning Loss    :  0.7316834412991714\n",
      "-------------Round number:  23  -------------\n",
      "Average Test Accuracy          :  0.8050478677110531\n",
      "Average Global Trainning Accuracy:  0.8461553352950286\n",
      "Average Global Trainning Loss    :  0.7011485839011925\n",
      "-------------Round number:  24  -------------\n",
      "Average Test Accuracy          :  0.8102117783579924\n",
      "Average Global Trainning Accuracy:  0.8520017035775128\n",
      "Average Global Trainning Loss    :  0.6727863020510686\n",
      "-------------Round number:  25  -------------\n",
      "Average Test Accuracy          :  0.8151436031331593\n",
      "Average Global Trainning Accuracy:  0.8568994889267462\n",
      "Average Global Trainning Loss    :  0.6463465189184219\n",
      "-------------Round number:  26  -------------\n",
      "Average Test Accuracy          :  0.8193211488250652\n",
      "Average Global Trainning Accuracy:  0.8609648443549636\n",
      "Average Global Trainning Loss    :  0.6209737025345555\n",
      "-------------Round number:  27  -------------\n",
      "Average Test Accuracy          :  0.8231505657093124\n",
      "Average Global Trainning Accuracy:  0.8659787827164318\n",
      "Average Global Trainning Loss    :  0.5970970745774934\n",
      "-------------Round number:  28  -------------\n",
      "Average Test Accuracy          :  0.8279083260806498\n",
      "Average Global Trainning Accuracy:  0.8708378503949202\n",
      "Average Global Trainning Loss    :  0.5755901646831927\n",
      "-------------Round number:  29  -------------\n",
      "Average Test Accuracy          :  0.8315636785610676\n",
      "Average Global Trainning Accuracy:  0.8746515409632957\n",
      "Average Global Trainning Loss    :  0.5556002388396314\n",
      "-------------Round number:  30  -------------\n",
      "Average Test Accuracy          :  0.8361473745285756\n",
      "Average Global Trainning Accuracy:  0.8783684373548087\n",
      "Average Global Trainning Loss    :  0.5370523949904173\n",
      "-------------Round number:  31  -------------\n",
      "Average Test Accuracy          :  0.8382361473745286\n",
      "Average Global Trainning Accuracy:  0.8815045686851479\n",
      "Average Global Trainning Loss    :  0.5190192617387138\n",
      "-------------Round number:  32  -------------\n",
      "Average Test Accuracy          :  0.8409051348999129\n",
      "Average Global Trainning Accuracy:  0.8844664704971349\n",
      "Average Global Trainning Loss    :  0.5026140477388881\n",
      "-------------Round number:  33  -------------\n",
      "Average Test Accuracy          :  0.8441543371047288\n",
      "Average Global Trainning Accuracy:  0.8875638841567292\n",
      "Average Global Trainning Loss    :  0.48701301942175157\n",
      "-------------Round number:  34  -------------\n",
      "Average Test Accuracy          :  0.8463011314186248\n",
      "Average Global Trainning Accuracy:  0.8909903980176552\n",
      "Average Global Trainning Loss    :  0.4723341068801301\n",
      "-------------Round number:  35  -------------\n",
      "Average Test Accuracy          :  0.847983753988976\n",
      "Average Global Trainning Accuracy:  0.8937587114759176\n",
      "Average Global Trainning Loss    :  0.4583797516866385\n",
      "-------------Round number:  36  -------------\n",
      "Average Test Accuracy          :  0.8504206556425877\n",
      "Average Global Trainning Accuracy:  0.8958107480253988\n",
      "Average Global Trainning Loss    :  0.4458353776061832\n",
      "-------------Round number:  37  -------------\n",
      "Average Test Accuracy          :  0.8523353640847113\n",
      "Average Global Trainning Accuracy:  0.8984241908006815\n",
      "Average Global Trainning Loss    :  0.4336281194948312\n",
      "-------------Round number:  38  -------------\n",
      "Average Test Accuracy          :  0.8535538149115173\n",
      "Average Global Trainning Accuracy:  0.9005730215270249\n",
      "Average Global Trainning Loss    :  0.42234649350995046\n",
      "-------------Round number:  39  -------------\n",
      "Average Test Accuracy          :  0.8560487380330722\n",
      "Average Global Trainning Accuracy:  0.9028186464302308\n",
      "Average Global Trainning Loss    :  0.4113978621796113\n",
      "-------------Round number:  40  -------------\n",
      "Average Test Accuracy          :  0.8565129097766173\n",
      "Average Global Trainning Accuracy:  0.9052191420164163\n",
      "Average Global Trainning Loss    :  0.40066406854963604\n",
      "-------------Round number:  41  -------------\n",
      "Average Test Accuracy          :  0.8579634464751958\n",
      "Average Global Trainning Accuracy:  0.9071550255536627\n",
      "Average Global Trainning Loss    :  0.3906325998552927\n",
      "-------------Round number:  42  -------------\n",
      "Average Test Accuracy          :  0.8588337684943429\n",
      "Average Global Trainning Accuracy:  0.9088392442310671\n",
      "Average Global Trainning Loss    :  0.38121955520655876\n",
      "-------------Round number:  43  -------------\n",
      "Average Test Accuracy          :  0.8600522193211488\n",
      "Average Global Trainning Accuracy:  0.9106396159207062\n",
      "Average Global Trainning Loss    :  0.37240050919786666\n",
      "-------------Round number:  44  -------------\n",
      "Average Test Accuracy          :  0.8613286916158979\n",
      "Average Global Trainning Accuracy:  0.9123238345981106\n",
      "Average Global Trainning Loss    :  0.36385823403864026\n",
      "-------------Round number:  45  -------------\n",
      "Average Test Accuracy          :  0.8627212068465332\n",
      "Average Global Trainning Accuracy:  0.9141435651231222\n",
      "Average Global Trainning Loss    :  0.3556632457991327\n",
      "-------------Round number:  46  -------------\n",
      "Average Test Accuracy          :  0.8632434000580215\n",
      "Average Global Trainning Accuracy:  0.9155180424345671\n",
      "Average Global Trainning Loss    :  0.3476680467903051\n",
      "-------------Round number:  47  -------------\n",
      "Average Test Accuracy          :  0.8639976791412822\n",
      "Average Global Trainning Accuracy:  0.9172603376180889\n",
      "Average Global Trainning Loss    :  0.3399435311846639\n",
      "-------------Round number:  48  -------------\n",
      "Average Test Accuracy          :  0.8648680011604294\n",
      "Average Global Trainning Accuracy:  0.918518661917299\n",
      "Average Global Trainning Loss    :  0.3329031537962676\n",
      "-------------Round number:  49  -------------\n",
      "Average Test Accuracy          :  0.865564258775747\n",
      "Average Global Trainning Accuracy:  0.9197769862165092\n",
      "Average Global Trainning Loss    :  0.3260179117624284\n",
      "-------------Round number:  50  -------------\n",
      "Average Test Accuracy          :  0.8664926022628372\n",
      "Average Global Trainning Accuracy:  0.921228898869444\n",
      "Average Global Trainning Loss    :  0.31946713898191886\n",
      "-------------Round number:  51  -------------\n",
      "Average Test Accuracy          :  0.8666666666666667\n",
      "Average Global Trainning Accuracy:  0.922603376180889\n",
      "Average Global Trainning Loss    :  0.31306635936532057\n",
      "-------------Round number:  52  -------------\n",
      "Average Test Accuracy          :  0.8678270960255294\n",
      "Average Global Trainning Accuracy:  0.9241520830106861\n",
      "Average Global Trainning Loss    :  0.30664251551126687\n",
      "-------------Round number:  53  -------------\n",
      "Average Test Accuracy          :  0.869335654192051\n",
      "Average Global Trainning Accuracy:  0.9255072014867586\n",
      "Average Global Trainning Loss    :  0.3007290340788098\n",
      "-------------Round number:  54  -------------\n",
      "Average Test Accuracy          :  0.8699158688714824\n",
      "Average Global Trainning Accuracy:  0.9270171906458108\n",
      "Average Global Trainning Loss    :  0.29472227482044683\n",
      "-------------Round number:  55  -------------\n",
      "Average Test Accuracy          :  0.870554105018857\n",
      "Average Global Trainning Accuracy:  0.9284110267926281\n",
      "Average Global Trainning Loss    :  0.28933813654270557\n",
      "-------------Round number:  56  -------------\n",
      "Average Test Accuracy          :  0.8714824485059472\n",
      "Average Global Trainning Accuracy:  0.929533839244231\n",
      "Average Global Trainning Loss    :  0.28399891681266454\n",
      "-------------Round number:  57  -------------\n",
      "Average Test Accuracy          :  0.8718885987815491\n",
      "Average Global Trainning Accuracy:  0.9307728047080688\n",
      "Average Global Trainning Loss    :  0.2788475004718716\n",
      "-------------Round number:  58  -------------\n",
      "Average Test Accuracy          :  0.8727589208006963\n",
      "Average Global Trainning Accuracy:  0.931992411336534\n",
      "Average Global Trainning Loss    :  0.2739576930801843\n",
      "-------------Round number:  59  -------------\n",
      "Average Test Accuracy          :  0.8731650710762983\n",
      "Average Global Trainning Accuracy:  0.9329603531051572\n",
      "Average Global Trainning Loss    :  0.2691538850460256\n",
      "-------------Round number:  60  -------------\n",
      "Average Test Accuracy          :  0.8728749637365826\n",
      "Average Global Trainning Accuracy:  0.934121883227505\n",
      "Average Global Trainning Loss    :  0.26435567292521683\n",
      "-------------Round number:  61  -------------\n",
      "Average Test Accuracy          :  0.8740353930954453\n",
      "Average Global Trainning Accuracy:  0.9355737958804399\n",
      "Average Global Trainning Loss    :  0.2597614847803256\n",
      "-------------Round number:  62  -------------\n",
      "Average Test Accuracy          :  0.8744415433710473\n",
      "Average Global Trainning Accuracy:  0.9365804553198079\n",
      "Average Global Trainning Loss    :  0.2554691545694111\n",
      "-------------Round number:  63  -------------\n",
      "Average Test Accuracy          :  0.875137800986365\n",
      "Average Global Trainning Accuracy:  0.9375677559238036\n",
      "Average Global Trainning Loss    :  0.25118259041786045\n",
      "-------------Round number:  64  -------------\n",
      "Average Test Accuracy          :  0.8755439512619669\n",
      "Average Global Trainning Accuracy:  0.9389228743998761\n",
      "Average Global Trainning Loss    :  0.24687751816100745\n",
      "-------------Round number:  65  -------------\n",
      "Average Test Accuracy          :  0.8755439512619669\n",
      "Average Global Trainning Accuracy:  0.9397746631562646\n",
      "Average Global Trainning Loss    :  0.24277590272669197\n",
      "-------------Round number:  66  -------------\n",
      "Average Test Accuracy          :  0.8769364664926023\n",
      "Average Global Trainning Accuracy:  0.9404135047235558\n",
      "Average Global Trainning Loss    :  0.23884437659105429\n",
      "-------------Round number:  67  -------------\n",
      "Average Test Accuracy          :  0.8763562518131709\n",
      "Average Global Trainning Accuracy:  0.9417299055288834\n",
      "Average Global Trainning Loss    :  0.2349150623535988\n",
      "-------------Round number:  68  -------------\n",
      "Average Test Accuracy          :  0.876820423556716\n",
      "Average Global Trainning Accuracy:  0.942504258943782\n",
      "Average Global Trainning Loss    :  0.23115922142998876\n",
      "-------------Round number:  69  -------------\n",
      "Average Test Accuracy          :  0.8768784450246592\n",
      "Average Global Trainning Accuracy:  0.9436851479015023\n",
      "Average Global Trainning Loss    :  0.227431416788466\n",
      "-------------Round number:  70  -------------\n",
      "Average Test Accuracy          :  0.8775166811720336\n",
      "Average Global Trainning Accuracy:  0.9446918073408703\n",
      "Average Global Trainning Loss    :  0.2238363071327629\n",
      "-------------Round number:  71  -------------\n",
      "Average Test Accuracy          :  0.8778648099796925\n",
      "Average Global Trainning Accuracy:  0.9457371844509834\n",
      "Average Global Trainning Loss    :  0.22035901413920939\n",
      "-------------Round number:  72  -------------\n",
      "Average Test Accuracy          :  0.8776327241079199\n",
      "Average Global Trainning Accuracy:  0.9466083320427443\n",
      "Average Global Trainning Loss    :  0.2169851510170648\n",
      "-------------Round number:  73  -------------\n",
      "Average Test Accuracy          :  0.8779228314476356\n",
      "Average Global Trainning Accuracy:  0.9474988384698777\n",
      "Average Global Trainning Loss    :  0.21372696827933832\n",
      "-------------Round number:  74  -------------\n",
      "Average Test Accuracy          :  0.8784450246591239\n",
      "Average Global Trainning Accuracy:  0.9484861390738734\n",
      "Average Global Trainning Loss    :  0.21046984422792125\n",
      "-------------Round number:  75  -------------\n",
      "Average Test Accuracy          :  0.8776327241079199\n",
      "Average Global Trainning Accuracy:  0.9493572866656342\n",
      "Average Global Trainning Loss    :  0.20711227731289686\n",
      "-------------Round number:  76  -------------\n",
      "Average Test Accuracy          :  0.8786190890629533\n",
      "Average Global Trainning Accuracy:  0.9503445872696299\n",
      "Average Global Trainning Loss    :  0.20406428161781787\n",
      "-------------Round number:  77  -------------\n",
      "Average Test Accuracy          :  0.8787931534667827\n",
      "Average Global Trainning Accuracy:  0.9513706055443705\n",
      "Average Global Trainning Loss    :  0.20094679072856977\n",
      "-------------Round number:  78  -------------\n",
      "Average Test Accuracy          :  0.8787351319988396\n",
      "Average Global Trainning Accuracy:  0.9520868824531517\n",
      "Average Global Trainning Loss    :  0.19806567046301493\n",
      "-------------Round number:  79  -------------\n",
      "Average Test Accuracy          :  0.8794313896141572\n",
      "Average Global Trainning Accuracy:  0.9523385473129936\n",
      "Average Global Trainning Loss    :  0.19541031505294643\n",
      "-------------Round number:  80  -------------\n",
      "Average Test Accuracy          :  0.8791412822744415\n",
      "Average Global Trainning Accuracy:  0.9536936657890661\n",
      "Average Global Trainning Loss    :  0.19236524965033103\n",
      "-------------Round number:  81  -------------\n",
      "Average Test Accuracy          :  0.8796054540179866\n",
      "Average Global Trainning Accuracy:  0.9544873780393371\n",
      "Average Global Trainning Loss    :  0.1895288588813497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/abdulmomen96/sophia/6cb134ba49b44524b530029a7e3e5858\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     glob_acc [82]   : (0.10809399477806789, 0.8796054540179866)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [2460]     : (0.017132019624114037, 2.5103986263275146)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_acc [82]  : (0.10628000619482732, 0.9544873780393371)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss [82] : (0.1895288588813497, 2.3275186752264982)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (621.00 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[36m────────────────────────────────────────────── \u001b[0m\u001b[1;36mNew Comet feature!\u001b[0m\u001b[36m ───────────────────────────────────────────────\u001b[0m\n",
      "Log your models to better track, deploy, share, and reproduce your work using: 'comet_ml.integration.pytorch.log_model'.\n",
      "Learn more at: https://comet.com/docs/v2/pytorch_log_model\n",
      "\n",
      "Hide this message by setting environment variable \"COMET_DISABLE_ANNOUNCEMENT=1\" \n",
      "\u001b[36m─────────────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **FedProx)\n",
    "experiment.end()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophiaOTA_params)\n",
    "experiment.end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophia_params)\n",
    "experiment.end()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Human Activity\n",
    "Sophia\n",
    "-------------Round number:  40  -------------\n",
    "Average Test Accuracy          :  0.8502321981424149\n",
    "Average Global Trainning Accuracy:  0.8495139338950097\n",
    "Average Global Trainning Loss    :  1.0823885845349968\n",
    "Clipping rate = 0.576000928907216\n",
    "\n",
    "\"learning_rate\": 0.0001,\n",
    "    \"alpha\": (0.9, 0.95),\n",
    "    \"eta\": 1.0 * 50,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 20,\n",
    "\n",
    "\n",
    "GD\n",
    "-------------Round number:  40  -------------\n",
    "Average Global Accuracy          :  0.20936532507739938\n",
    "Average Global Trainning Accuracy:  0.20764744005184704\n",
    "Average Global Trainning Loss    :  1.7220779072828905\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "MNIST MLP\n",
    "Sophia \n",
    "-------------Round number:  39  -------------\n",
    "Average Global Accuracy          :  0.8934867045115028\n",
    "Average Global Trainning Accuracy:  0.8913894080996885\n",
    "Average Global Trainning Loss    :  0.3557637022975078\n",
    "\n",
    "GD\n",
    "-------------Round number:  40  -------------\n",
    "Average Global Accuracy          :  0.5679713175978488\n",
    "Average Global Trainning Accuracy:  0.5671775700934579\n",
    "Average Global Trainning Loss    :  1.9333872663551401\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 421642,  Total Parameters: 421642\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in model.parameters()\n",
    ")\n",
    "\n",
    "\n",
    "trainable_params = sum(\n",
    "\tparam.numel() for param in model.parameters() if param.requires_grad\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params},  Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[288, 32, 18432, 64, 401408, 128, 1280, 10]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_params = [param.numel() for param in model.parameters() if param.requires_grad]\n",
    "trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 3136])\n",
      "torch.Size([128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401408\n",
      "128\n",
      "torch.Size([128, 3136])\n",
      "torch.Size([128])\n",
      "1280\n",
      "10\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10])\n",
      "Total Parameters: 402826\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "total_params = 0\n",
    "for layer in model.children():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        total_params += layer.state_dict()['weight'].numel() + layer.state_dict()['bias'].numel()\n",
    "        print(layer.state_dict()['weight'].numel())\n",
    "        print(layer.state_dict()['bias'].numel())\n",
    "        print(layer.state_dict()['weight'].shape)\n",
    "        print(layer.state_dict()['bias'].shape)\n",
    "        \n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import get_training_data_value, simple_read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rs_train_acc_sop, rs_train_loss_sop, rs_glob_acc_sop = simple_read_data(\"Mnist_Sophia_0.01_(0.9, 0.95)_20.0_0.1_32u_64b_1_0_25db\")\n",
    "rs_train_acc_done, rs_train_loss_done, rs_glob_acc_done = simple_read_data(\"Mnist_DONE_1_0.003_1.0_0.001_32u_0b_40_0\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_gd, rs_train_loss_gd, rs_glob_acc_gd = simple_read_data(\"Mnist_GD_0.01_0.03_20.0_0.001_30u_64b_1_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "print(rs_glob_acc_sop[:80].shape)\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop[:80], label='FedSophia-25dB', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_glob_acc_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "#plt.plot(x, rs_glob_acc_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop[:80], label='FedSophia-25dB', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_train_loss_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "#plt.plot(x, rs_train_loss_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rs_train_acc_sop, rs_train_loss_sop, rs_glob_acc_sop = simple_read_data(\"Fashion_Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.1_20u_64b_1_0\")\n",
    "#rs_train_acc_done, rs_train_loss_done, rs_glob_acc_done = simple_read_data(\"Mnist_DONE_1_0.003_1.0_0.001_32u_0b_40_0\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_gd, rs_train_loss_gd, rs_glob_acc_gd = simple_read_data(\"Fashion_Mnist_GD_0.01_0.03_20.0_0.001_20u_64b_1_0\")\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "print(rs_glob_acc_sop[:80].shape)\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_glob_acc_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_glob_acc_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_train_loss_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_train_loss_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rs_train_acc_sop, rs_train_loss_sop, rs_glob_acc_sop = simple_read_data(\"Mnist_Sophia_0.005_(0.9, 0.95)_20.0_0.1_32u_64b_1_0_CNN_IID\")\n",
    "#rs_train_acc_done, rs_train_loss_done, rs_glob_acc_done = simple_read_data(\"Mnist_DONE_1_0.003_1.0_0.001_32u_0b_40_0\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_gd, rs_train_loss_gd, rs_glob_acc_gd = simple_read_data(\"Mnist_GD_0.01_0.03_20.0_0.001_30u_64b_1_0_CNN_IID\")\n",
    "\n",
    "b = rs_train_loss_gd\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "print(rs_glob_acc_sop[:80].shape)\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_glob_acc_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_glob_acc_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_train_loss_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_train_loss_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rs_train_acc_sop, rs_train_loss_sop, rs_glob_acc_sop = simple_read_data(\"Mnist_Sophia_0.005_(0.9, 0.95)_20.0_0.1_32u_64b_1_0_CNN_NIID\")\n",
    "#rs_train_acc_done, rs_train_loss_done, rs_glob_acc_done = simple_read_data(\"Mnist_DONE_1_0.003_1.0_0.001_32u_0b_40_0\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_gd, rs_train_loss_gd, rs_glob_acc_gd = simple_read_data(\"Mnist_GD_0.01_0.03_20.0_0.001_30u_64b_1_0_CNN_NIID\")\n",
    "\n",
    "a = rs_train_loss_gd\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "print(rs_glob_acc_sop[:80].shape)\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_glob_acc_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_glob_acc_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_train_loss_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_train_loss_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sopgia with SNRs\n",
    "\n",
    "from utils.plot_utils import get_training_data_value, simple_read_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rs_train_acc_sop25, rs_train_loss_sop25, rs_glob_acc_sop25 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_25dB\")\n",
    "rs_train_acc_sop30, rs_train_loss_sop30, rs_glob_acc_sop30 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_30dB\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_sop35, rs_train_loss_sop35, rs_glob_acc_sop35 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_35dB\")\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop25[:80], label='SNR=25dB', linestyle='--', marker='o', markevery=4)\n",
    "plt.plot(x[:80], rs_glob_acc_sop30[:80], label='SNR=30dB', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x[:80], rs_glob_acc_sop35[:80], label='SNR=35dB', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop25[:80], label='SNR=25dB', linestyle='--', marker='o', markevery=4)\n",
    "plt.plot(x[:80], rs_train_loss_sop30[:80], label='SNR=30dB', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x[:80], rs_train_loss_sop35[:80], label='SNR=35dB', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sopgia with SNRs\n",
    "\n",
    "from utils.plot_utils import get_training_data_value, simple_read_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rs_train_acc_sop25, rs_train_loss_sop25, rs_glob_acc_sop25 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_fix_5dB\")\n",
    "rs_train_acc_sop30, rs_train_loss_sop30, rs_glob_acc_sop30 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_fix_25dB\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_sop35, rs_train_loss_sop35, rs_glob_acc_sop35 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_fix_35dB\")\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop25[:80], label='SNR=5dB', linestyle='--', marker='o', markevery=4)\n",
    "plt.plot(x[:80], rs_glob_acc_sop30[:80], label='SNR=25dB', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x[:80], rs_glob_acc_sop35[:80], label='SNR=35dB', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop25[:80], label='SNR=5dB', linestyle='--', marker='o', markevery=4)\n",
    "plt.plot(x[:80], rs_train_loss_sop30[:80], label='SNR=25dB', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x[:80], rs_train_loss_sop35[:80], label='SNR=35dB', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.027\n",
    "\n",
    "np.mean(np.equal(np.ones((20, 1), np.float32), 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN( input_dim = 123, output_dim = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        #print(grads[i].sign())\n",
    "#print(f\"Clipping rate = {1 - (winrate / param_count)}\")\n",
    "x = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mnist MLP settings\n",
    "sophia_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20,\n",
    "    \"L\": 0.1,\n",
    "    \"rho\": 20,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.003,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 41,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "GD_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"GD\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 2, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(18432, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(2, 1)(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(2, 1)(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x#F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 421642\n"
     ]
    }
   ],
   "source": [
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {param_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN28(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN28, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Adjusted input size based on the dimensions after pooling\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 79510\n"
     ]
    }
   ],
   "source": [
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {param_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "351.36833333333334"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "421642 / 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet18 and Cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from comet_ml import Experiment\n",
    "#\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "def main(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters,\n",
    "         local_epochs, optimizer, numedges, times, commet, gpu):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "    for i in range(times):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            if(dataset == \"human_activity\"):\n",
    "                model = Mclr_CrossEntropy(561,6).to(device), model\n",
    "                print(\"Hey\")\n",
    "            else:\n",
    "                model = Mclr_Logistic().to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(40,1).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(300).to(device), model\n",
    "        \n",
    "        if model == \"MLP\" and dataset == \"a9a\":\n",
    "            model = DNN( input_dim = 123, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"human_activity\":\n",
    "            model = DNN( input_dim = 561, mid_dim = 561, output_dim = 6).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"w8a\":\n",
    "            model = DNN( input_dim = 300, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"Mnist\":\n",
    "            model = DNN().to(device), model\n",
    "        if model == 'CNN':\n",
    "            model = Net().to(device), model\n",
    "        # select algorithm\n",
    "        if model == \"Resnet\":\n",
    "            model = resnet18().to(device), model\n",
    "        if(commet):\n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\")\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNewton_params = {\\n    \"dataset\": \"Cifar100\",\\n    \"algorithm\": \"Newton\",\\n    \"model\": \"MLP\",\\n    \"batch_size\": 0,\\n    \"learning_rate\": 1,\\n    \"alpha\": 0.03,\\n    \"eta\": 1.0,\\n    \"L\": 1e-3,\\n    \"rho\": 0.01,\\n    \"num_glob_iters\": 500,\\n    \"local_epochs\": 40,\\n    \"optimizer\": \"Newton\",\\n    \"numedges\": 30,\\n    \"times\": 1,\\n    \"commet\": True,\\n    \"gpu\": 0\\n}\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sophiaOTA_params = {\n",
    "    \"dataset\": \"human_activity\",\n",
    "    \"algorithm\": \"Sophia_OTA\",\n",
    "    \"model\": \"mclr\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 1280,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "sophia_params = {\n",
    "    \"dataset\": \"human_activity\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"mclr\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.003,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 41,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "GD_params = {\n",
    "    \"dataset\": \"human_activity\",\n",
    "    \"algorithm\": \"GD\",\n",
    "    \"model\": \"mclr\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "'''\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/abdulmomen96/sophia/654b7c15d2e8401a80addf9c94d2c20a\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------ 0\n",
      "Hey\n",
      "Number of edges / total edges: 32  /  30\n",
      "Total number of parameters: 3372\n",
      "-------------Round number:  0  -------------\n",
      "Win rate = 0.010083036773428256\n",
      "-------------Round number:  1  -------------\n",
      "Average Test Accuracy          :  0.15982972136222912\n",
      "Average Global Trainning Accuracy:  0.15800388852883993\n",
      "Average Global Trainning Loss    :  1.8359506642903436\n",
      "Win rate = 0.7938908659549229\n",
      "-------------Round number:  2  -------------\n",
      "Average Test Accuracy          :  0.26509287925696595\n",
      "Average Global Trainning Accuracy:  0.26351263771872974\n",
      "Average Global Trainning Loss    :  1.6244059613982502\n",
      "Win rate = 0.8131672597864769\n",
      "-------------Round number:  3  -------------\n",
      "Average Test Accuracy          :  0.40789473684210525\n",
      "Average Global Trainning Accuracy:  0.40622164614387557\n",
      "Average Global Trainning Loss    :  1.5063322768146468\n",
      "Win rate = 0.8437129300118624\n",
      "-------------Round number:  4  -------------\n",
      "Average Test Accuracy          :  0.45859133126934987\n",
      "Average Global Trainning Accuracy:  0.4684381075826312\n",
      "Average Global Trainning Loss    :  1.4348299274951393\n",
      "Win rate = 0.8472716488730724\n",
      "-------------Round number:  5  -------------\n",
      "Average Test Accuracy          :  0.5812693498452013\n",
      "Average Global Trainning Accuracy:  0.5997407647440052\n",
      "Average Global Trainning Loss    :  1.3664609324368113\n",
      "Win rate = 0.8668446026097272\n",
      "-------------Round number:  6  -------------\n",
      "Average Test Accuracy          :  0.596749226006192\n",
      "Average Global Trainning Accuracy:  0.5968891769280622\n",
      "Average Global Trainning Loss    :  1.291847127146792\n",
      "Win rate = 0.9448398576512456\n",
      "-------------Round number:  7  -------------\n",
      "Average Test Accuracy          :  0.6234520123839009\n",
      "Average Global Trainning Accuracy:  0.6282566429034349\n",
      "Average Global Trainning Loss    :  1.2393513549092676\n",
      "Win rate = 0.9495848161328588\n",
      "-------------Round number:  8  -------------\n",
      "Average Test Accuracy          :  0.6625386996904025\n",
      "Average Global Trainning Accuracy:  0.668697342838626\n",
      "Average Global Trainning Loss    :  1.1951957935029165\n",
      "Win rate = 0.9531435349940688\n",
      "-------------Round number:  9  -------------\n",
      "Average Test Accuracy          :  0.6594427244582043\n",
      "Average Global Trainning Accuracy:  0.6729747245625405\n",
      "Average Global Trainning Loss    :  1.168806581132534\n",
      "Win rate = 0.9584816132858838\n",
      "-------------Round number:  10  -------------\n",
      "Average Test Accuracy          :  0.6803405572755418\n",
      "Average Global Trainning Accuracy:  0.691250810110175\n",
      "Average Global Trainning Loss    :  1.133468689241737\n",
      "Win rate = 0.9635231316725978\n",
      "-------------Round number:  11  -------------\n",
      "Average Test Accuracy          :  0.6876934984520123\n",
      "Average Global Trainning Accuracy:  0.6996759559300065\n",
      "Average Global Trainning Loss    :  1.1054122954471808\n",
      "Win rate = 0.9780545670225386\n",
      "-------------Round number:  12  -------------\n",
      "Average Test Accuracy          :  0.6996904024767802\n",
      "Average Global Trainning Accuracy:  0.708749189889825\n",
      "Average Global Trainning Loss    :  1.072279928507777\n",
      "Win rate = 0.9753855278766311\n",
      "-------------Round number:  13  -------------\n",
      "Average Test Accuracy          :  0.7000773993808049\n",
      "Average Global Trainning Accuracy:  0.7139338950097214\n",
      "Average Global Trainning Loss    :  1.0554305862159754\n",
      "Win rate = 0.9822064056939501\n",
      "-------------Round number:  14  -------------\n",
      "Average Test Accuracy          :  0.705108359133127\n",
      "Average Global Trainning Accuracy:  0.7209332469215813\n",
      "Average Global Trainning Loss    :  1.036999503807518\n",
      "Win rate = 0.9733096085409253\n",
      "-------------Round number:  15  -------------\n",
      "Average Test Accuracy          :  0.718266253869969\n",
      "Average Global Trainning Accuracy:  0.7345430978613091\n",
      "Average Global Trainning Loss    :  1.0176612878726508\n",
      "Win rate = 0.9756820877817319\n",
      "-------------Round number:  16  -------------\n",
      "Average Test Accuracy          :  0.7213622291021672\n",
      "Average Global Trainning Accuracy:  0.7410239792611795\n",
      "Average Global Trainning Loss    :  0.9966848139784511\n",
      "Win rate = 0.9851720047449585\n",
      "-------------Round number:  17  -------------\n",
      "Average Test Accuracy          :  0.7372291021671826\n",
      "Average Global Trainning Accuracy:  0.7541153596889177\n",
      "Average Global Trainning Loss    :  0.9798081937581011\n",
      "Win rate = 0.9822064056939501\n",
      "-------------Round number:  18  -------------\n",
      "Average Test Accuracy          :  0.7441950464396285\n",
      "Average Global Trainning Accuracy:  0.7577446532728451\n",
      "Average Global Trainning Loss    :  0.9656160761301037\n",
      "Win rate = 0.9854685646500593\n",
      "-------------Round number:  19  -------------\n",
      "Average Test Accuracy          :  0.7608359133126935\n",
      "Average Global Trainning Accuracy:  0.779650032404407\n",
      "Average Global Trainning Loss    :  0.9558754506237849\n",
      "Win rate = 0.9875444839857651\n",
      "-------------Round number:  20  -------------\n",
      "Average Test Accuracy          :  0.7708978328173375\n",
      "Average Global Trainning Accuracy:  0.7875567077122488\n",
      "Average Global Trainning Loss    :  0.9444801117952042\n",
      "Win rate = 0.9848754448398577\n",
      "-------------Round number:  21  -------------\n",
      "Average Test Accuracy          :  0.760061919504644\n",
      "Average Global Trainning Accuracy:  0.7756318859364874\n",
      "Average Global Trainning Loss    :  0.9317366990035645\n",
      "Win rate = 0.9851720047449585\n",
      "-------------Round number:  22  -------------\n",
      "Average Test Accuracy          :  0.7786377708978328\n",
      "Average Global Trainning Accuracy:  0.7920933246921581\n",
      "Average Global Trainning Loss    :  0.9230072555492547\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  23  -------------\n",
      "Average Test Accuracy          :  0.7821207430340558\n",
      "Average Global Trainning Accuracy:  0.797796500324044\n",
      "Average Global Trainning Loss    :  0.9143149632412508\n",
      "Win rate = 0.9908066429418743\n",
      "-------------Round number:  24  -------------\n",
      "Average Test Accuracy          :  0.7925696594427245\n",
      "Average Global Trainning Accuracy:  0.8049254698639015\n",
      "Average Global Trainning Loss    :  0.9064021488172391\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  25  -------------\n",
      "Average Test Accuracy          :  0.7960526315789473\n",
      "Average Global Trainning Accuracy:  0.8075178224238496\n",
      "Average Global Trainning Loss    :  0.8989120473509398\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  26  -------------\n",
      "Average Test Accuracy          :  0.8041795665634675\n",
      "Average Global Trainning Accuracy:  0.8160725858716785\n",
      "Average Global Trainning Loss    :  0.8894641374351911\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  27  -------------\n",
      "Average Test Accuracy          :  0.8022445820433437\n",
      "Average Global Trainning Accuracy:  0.814646791963707\n",
      "Average Global Trainning Loss    :  0.8820932740602722\n",
      "Win rate = 0.9878410438908659\n",
      "-------------Round number:  28  -------------\n",
      "Average Test Accuracy          :  0.8072755417956656\n",
      "Average Global Trainning Accuracy:  0.8206092028515878\n",
      "Average Global Trainning Loss    :  0.8739973620787427\n",
      "Win rate = 0.9875444839857651\n",
      "-------------Round number:  29  -------------\n",
      "Average Test Accuracy          :  0.8057275541795665\n",
      "Average Global Trainning Accuracy:  0.819053791315619\n",
      "Average Global Trainning Loss    :  0.8685437383546663\n",
      "Win rate = 0.9881376037959668\n",
      "-------------Round number:  30  -------------\n",
      "Average Test Accuracy          :  0.8130804953560371\n",
      "Average Global Trainning Accuracy:  0.8264419961114712\n",
      "Average Global Trainning Loss    :  0.8637904169029488\n",
      "Win rate = 0.9922894424673784\n",
      "-------------Round number:  31  -------------\n",
      "Average Test Accuracy          :  0.820046439628483\n",
      "Average Global Trainning Accuracy:  0.8370706416072586\n",
      "Average Global Trainning Loss    :  0.8550742643186974\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  32  -------------\n",
      "Average Test Accuracy          :  0.8243034055727554\n",
      "Average Global Trainning Accuracy:  0.8430330524951394\n",
      "Average Global Trainning Loss    :  0.8475128984729423\n",
      "Win rate = 0.991399762752076\n",
      "-------------Round number:  33  -------------\n",
      "Average Test Accuracy          :  0.8250773993808049\n",
      "Average Global Trainning Accuracy:  0.8426441996111471\n",
      "Average Global Trainning Loss    :  0.8403660811932923\n",
      "Win rate = 0.9872479240806643\n",
      "-------------Round number:  34  -------------\n",
      "Average Test Accuracy          :  0.8208204334365325\n",
      "Average Global Trainning Accuracy:  0.8373298768632534\n",
      "Average Global Trainning Loss    :  0.8344404417125729\n",
      "Win rate = 0.9925860023724793\n",
      "-------------Round number:  35  -------------\n",
      "Average Test Accuracy          :  0.8173374613003096\n",
      "Average Global Trainning Accuracy:  0.8344782890473105\n",
      "Average Global Trainning Loss    :  0.8281935429155866\n",
      "Win rate = 0.991399762752076\n",
      "-------------Round number:  36  -------------\n",
      "Average Test Accuracy          :  0.8188854489164087\n",
      "Average Global Trainning Accuracy:  0.8370706416072586\n",
      "Average Global Trainning Loss    :  0.8214272242587491\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  37  -------------\n",
      "Average Test Accuracy          :  0.8293343653250774\n",
      "Average Global Trainning Accuracy:  0.8452365521710953\n",
      "Average Global Trainning Loss    :  0.8157173399019767\n",
      "Win rate = 0.9881376037959668\n",
      "-------------Round number:  38  -------------\n",
      "Average Test Accuracy          :  0.8335913312693498\n",
      "Average Global Trainning Accuracy:  0.8500324044069993\n",
      "Average Global Trainning Loss    :  0.8102240967271549\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  39  -------------\n",
      "Average Test Accuracy          :  0.8277863777089783\n",
      "Average Global Trainning Accuracy:  0.8449773169151005\n",
      "Average Global Trainning Loss    :  0.8066598651166559\n",
      "Win rate = 0.9884341637010676\n",
      "-------------Round number:  40  -------------\n",
      "Average Test Accuracy          :  0.8405572755417957\n",
      "Average Global Trainning Accuracy:  0.8532728451069346\n",
      "Average Global Trainning Loss    :  0.8030244324165586\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  41  -------------\n",
      "Average Test Accuracy          :  0.8397832817337462\n",
      "Average Global Trainning Accuracy:  0.8574206092028516\n",
      "Average Global Trainning Loss    :  0.7969402518430007\n",
      "Win rate = 0.986061684460261\n",
      "-------------Round number:  42  -------------\n",
      "Average Test Accuracy          :  0.8386222910216719\n",
      "Average Global Trainning Accuracy:  0.855346727154893\n",
      "Average Global Trainning Loss    :  0.7910454345228451\n",
      "Win rate = 0.9884341637010676\n",
      "-------------Round number:  43  -------------\n",
      "Average Test Accuracy          :  0.846749226006192\n",
      "Average Global Trainning Accuracy:  0.8600129617627997\n",
      "Average Global Trainning Loss    :  0.787303484992709\n",
      "Win rate = 0.9872479240806643\n",
      "-------------Round number:  44  -------------\n",
      "Average Test Accuracy          :  0.8494582043343654\n",
      "Average Global Trainning Accuracy:  0.861697990926766\n",
      "Average Global Trainning Loss    :  0.7853223858757291\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  45  -------------\n",
      "Average Test Accuracy          :  0.8595201238390093\n",
      "Average Global Trainning Accuracy:  0.8683084899546338\n",
      "Average Global Trainning Loss    :  0.7814674006602398\n",
      "Win rate = 0.991399762752076\n",
      "-------------Round number:  46  -------------\n",
      "Average Test Accuracy          :  0.8575851393188855\n",
      "Average Global Trainning Accuracy:  0.8675307841866494\n",
      "Average Global Trainning Loss    :  0.7769203408538561\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  47  -------------\n",
      "Average Test Accuracy          :  0.8475232198142415\n",
      "Average Global Trainning Accuracy:  0.8622164614387556\n",
      "Average Global Trainning Loss    :  0.7704738891364226\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  48  -------------\n",
      "Average Test Accuracy          :  0.8517801857585139\n",
      "Average Global Trainning Accuracy:  0.8648088139987038\n",
      "Average Global Trainning Loss    :  0.766295049720512\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  49  -------------\n",
      "Average Test Accuracy          :  0.8510061919504643\n",
      "Average Global Trainning Accuracy:  0.8655865197666883\n",
      "Average Global Trainning Loss    :  0.7622460810920285\n",
      "Win rate = 0.986061684460261\n",
      "-------------Round number:  50  -------------\n",
      "Average Test Accuracy          :  0.8529411764705882\n",
      "Average Global Trainning Accuracy:  0.8662346079066753\n",
      "Average Global Trainning Loss    :  0.7578239554641931\n",
      "Win rate = 0.9869513641755635\n",
      "-------------Round number:  51  -------------\n",
      "Average Test Accuracy          :  0.8571981424148607\n",
      "Average Global Trainning Accuracy:  0.868049254698639\n",
      "Average Global Trainning Loss    :  0.753977261220026\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  52  -------------\n",
      "Average Test Accuracy          :  0.858359133126935\n",
      "Average Global Trainning Accuracy:  0.8715489306545691\n",
      "Average Global Trainning Loss    :  0.7501463261503565\n",
      "Win rate = 0.9911032028469751\n",
      "-------------Round number:  53  -------------\n",
      "Average Test Accuracy          :  0.8490712074303406\n",
      "Average Global Trainning Accuracy:  0.864549578742709\n",
      "Average Global Trainning Loss    :  0.7467892422431951\n",
      "Win rate = 0.9922894424673784\n",
      "-------------Round number:  54  -------------\n",
      "Average Test Accuracy          :  0.8452012383900929\n",
      "Average Global Trainning Accuracy:  0.8592352559948153\n",
      "Average Global Trainning Loss    :  0.7431731129496112\n",
      "Win rate = 0.9893238434163701\n",
      "-------------Round number:  55  -------------\n",
      "Average Test Accuracy          :  0.8591331269349846\n",
      "Average Global Trainning Accuracy:  0.8701231367465976\n",
      "Average Global Trainning Loss    :  0.7404094980354828\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  56  -------------\n",
      "Average Test Accuracy          :  0.8637770897832817\n",
      "Average Global Trainning Accuracy:  0.8753078418664938\n",
      "Average Global Trainning Loss    :  0.7352456785685353\n",
      "Win rate = 0.9884341637010676\n",
      "-------------Round number:  57  -------------\n",
      "Average Test Accuracy          :  0.858359133126935\n",
      "Average Global Trainning Accuracy:  0.8697342838626053\n",
      "Average Global Trainning Loss    :  0.7319984658538561\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  58  -------------\n",
      "Average Test Accuracy          :  0.8606811145510835\n",
      "Average Global Trainning Accuracy:  0.8745301360985094\n",
      "Average Global Trainning Loss    :  0.7287163804277381\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  59  -------------\n",
      "Average Test Accuracy          :  0.8630030959752322\n",
      "Average Global Trainning Accuracy:  0.8749189889825016\n",
      "Average Global Trainning Loss    :  0.7255771402098186\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  60  -------------\n",
      "Average Test Accuracy          :  0.8637770897832817\n",
      "Average Global Trainning Accuracy:  0.8768632534024627\n",
      "Average Global Trainning Loss    :  0.7223679646994491\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  61  -------------\n",
      "Average Test Accuracy          :  0.8602941176470589\n",
      "Average Global Trainning Accuracy:  0.8762151652624757\n",
      "Average Global Trainning Loss    :  0.7192165095390473\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  62  -------------\n",
      "Average Test Accuracy          :  0.8610681114551083\n",
      "Average Global Trainning Accuracy:  0.8733635774465327\n",
      "Average Global Trainning Loss    :  0.7162830246476021\n",
      "Win rate = 0.9875444839857651\n",
      "-------------Round number:  63  -------------\n",
      "Average Test Accuracy          :  0.8602941176470589\n",
      "Average Global Trainning Accuracy:  0.8729747245625405\n",
      "Average Global Trainning Loss    :  0.7125879096119572\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  64  -------------\n",
      "Average Test Accuracy          :  0.8622291021671826\n",
      "Average Global Trainning Accuracy:  0.8753078418664938\n",
      "Average Global Trainning Loss    :  0.7119831750243033\n",
      "Win rate = 0.9878410438908659\n",
      "-------------Round number:  65  -------------\n",
      "Average Test Accuracy          :  0.8688080495356038\n",
      "Average Global Trainning Accuracy:  0.8781594296824368\n",
      "Average Global Trainning Loss    :  0.7090496268430007\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  66  -------------\n",
      "Average Test Accuracy          :  0.8630030959752322\n",
      "Average Global Trainning Accuracy:  0.875048606610499\n",
      "Average Global Trainning Loss    :  0.7073789644766688\n",
      "Win rate = 0.986061684460261\n",
      "-------------Round number:  67  -------------\n",
      "Average Test Accuracy          :  0.8676470588235294\n",
      "Average Global Trainning Accuracy:  0.8777705767984446\n",
      "Average Global Trainning Loss    :  0.7032156943656838\n",
      "Win rate = 0.9925860023724793\n",
      "-------------Round number:  68  -------------\n",
      "Average Test Accuracy          :  0.8668730650154799\n",
      "Average Global Trainning Accuracy:  0.8808813998703824\n",
      "Average Global Trainning Loss    :  0.70050815426523\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  69  -------------\n",
      "Average Test Accuracy          :  0.8672600619195047\n",
      "Average Global Trainning Accuracy:  0.8789371354504213\n",
      "Average Global Trainning Loss    :  0.6986649004577122\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  70  -------------\n",
      "Average Test Accuracy          :  0.8664860681114551\n",
      "Average Global Trainning Accuracy:  0.8791963707064161\n",
      "Average Global Trainning Loss    :  0.6970294272521063\n",
      "Win rate = 0.9881376037959668\n",
      "-------------Round number:  71  -------------\n",
      "Average Test Accuracy          :  0.8668730650154799\n",
      "Average Global Trainning Accuracy:  0.8789371354504213\n",
      "Average Global Trainning Loss    :  0.6938055684948153\n",
      "Win rate = 0.9934756820877817\n",
      "-------------Round number:  72  -------------\n",
      "Average Test Accuracy          :  0.8691950464396285\n",
      "Average Global Trainning Accuracy:  0.8803629293583928\n",
      "Average Global Trainning Loss    :  0.6916837760247894\n",
      "Win rate = 0.9866548042704626\n",
      "-------------Round number:  73  -------------\n",
      "Average Test Accuracy          :  0.8695820433436533\n",
      "Average Global Trainning Accuracy:  0.8817887232663643\n",
      "Average Global Trainning Loss    :  0.6900337461519767\n",
      "Win rate = 0.984282325029656\n",
      "-------------Round number:  74  -------------\n",
      "Average Test Accuracy          :  0.8637770897832817\n",
      "Average Global Trainning Accuracy:  0.875048606610499\n",
      "Average Global Trainning Loss    :  0.6886975073922553\n",
      "Win rate = 0.9872479240806643\n",
      "-------------Round number:  75  -------------\n",
      "Average Test Accuracy          :  0.8672600619195047\n",
      "Average Global Trainning Accuracy:  0.8773817239144523\n",
      "Average Global Trainning Loss    :  0.6871423490157161\n",
      "Win rate = 0.9878410438908659\n",
      "-------------Round number:  76  -------------\n",
      "Average Test Accuracy          :  0.8703560371517027\n",
      "Average Global Trainning Accuracy:  0.8795852235904082\n",
      "Average Global Trainning Loss    :  0.6853529548768632\n",
      "Win rate = 0.9872479240806643\n",
      "-------------Round number:  77  -------------\n",
      "Average Test Accuracy          :  0.871517027863777\n",
      "Average Global Trainning Accuracy:  0.8819183408943616\n",
      "Average Global Trainning Loss    :  0.6837445064403759\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  78  -------------\n",
      "Average Test Accuracy          :  0.8730650154798761\n",
      "Average Global Trainning Accuracy:  0.8828256642903435\n",
      "Average Global Trainning Loss    :  0.6814948684583604\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  79  -------------\n",
      "Average Test Accuracy          :  0.8699690402476781\n",
      "Average Global Trainning Accuracy:  0.8820479585223591\n",
      "Average Global Trainning Loss    :  0.6797699666842191\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  80  -------------\n",
      "Average Test Accuracy          :  0.8796439628482973\n",
      "Average Global Trainning Accuracy:  0.8881399870382372\n",
      "Average Global Trainning Loss    :  0.6781130382169475\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  81  -------------\n",
      "Average Test Accuracy          :  0.881578947368421\n",
      "Average Global Trainning Accuracy:  0.8889176928062217\n",
      "Average Global Trainning Loss    :  0.6761633308692482\n",
      "Win rate = 0.9916963226571768\n",
      "-------------Round number:  82  -------------\n",
      "Average Test Accuracy          :  0.8769349845201239\n",
      "Average Global Trainning Accuracy:  0.8864549578742709\n",
      "Average Global Trainning Loss    :  0.6736793938350616\n",
      "Win rate = 0.9878410438908659\n",
      "-------------Round number:  83  -------------\n",
      "Average Test Accuracy          :  0.8734520123839009\n",
      "Average Global Trainning Accuracy:  0.8839922229423202\n",
      "Average Global Trainning Loss    :  0.6717393065456903\n",
      "Win rate = 0.9893238434163701\n",
      "-------------Round number:  84  -------------\n",
      "Average Test Accuracy          :  0.8738390092879257\n",
      "Average Global Trainning Accuracy:  0.8854180168502916\n",
      "Average Global Trainning Loss    :  0.6703621192482178\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  85  -------------\n",
      "Average Test Accuracy          :  0.8769349845201239\n",
      "Average Global Trainning Accuracy:  0.8865845755022683\n",
      "Average Global Trainning Loss    :  0.6677062869612768\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  86  -------------\n",
      "Average Test Accuracy          :  0.8773219814241486\n",
      "Average Global Trainning Accuracy:  0.8869734283862605\n",
      "Average Global Trainning Loss    :  0.6664332747691186\n",
      "Win rate = 0.9934756820877817\n",
      "-------------Round number:  87  -------------\n",
      "Average Test Accuracy          :  0.8757739938080495\n",
      "Average Global Trainning Accuracy:  0.8863253402462735\n",
      "Average Global Trainning Loss    :  0.6648343831011018\n",
      "Win rate = 0.9916963226571768\n",
      "-------------Round number:  88  -------------\n",
      "Average Test Accuracy          :  0.8777089783281734\n",
      "Average Global Trainning Accuracy:  0.8885288399222294\n",
      "Average Global Trainning Loss    :  0.6625117086236229\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  89  -------------\n",
      "Average Test Accuracy          :  0.8753869969040248\n",
      "Average Global Trainning Accuracy:  0.8865845755022683\n",
      "Average Global Trainning Loss    :  0.6606667459899547\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  90  -------------\n",
      "Average Test Accuracy          :  0.8730650154798761\n",
      "Average Global Trainning Accuracy:  0.8856772521062865\n",
      "Average Global Trainning Loss    :  0.6598548636989631\n",
      "Win rate = 0.9908066429418743\n",
      "-------------Round number:  91  -------------\n",
      "Average Test Accuracy          :  0.8726780185758514\n",
      "Average Global Trainning Accuracy:  0.884251458198315\n",
      "Average Global Trainning Loss    :  0.657199474441024\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  92  -------------\n",
      "Average Test Accuracy          :  0.8722910216718266\n",
      "Average Global Trainning Accuracy:  0.8856772521062865\n",
      "Average Global Trainning Loss    :  0.6552733742101425\n",
      "Win rate = 0.9919928825622776\n",
      "-------------Round number:  93  -------------\n",
      "Average Test Accuracy          :  0.8719040247678018\n",
      "Average Global Trainning Accuracy:  0.8824368114063512\n",
      "Average Global Trainning Loss    :  0.6541520045163642\n",
      "Win rate = 0.9922894424673784\n",
      "-------------Round number:  94  -------------\n",
      "Average Test Accuracy          :  0.8726780185758514\n",
      "Average Global Trainning Accuracy:  0.8843810758263124\n",
      "Average Global Trainning Loss    :  0.6526214026045042\n",
      "Win rate = 0.9911032028469751\n",
      "-------------Round number:  95  -------------\n",
      "Average Test Accuracy          :  0.8730650154798761\n",
      "Average Global Trainning Accuracy:  0.8803629293583928\n",
      "Average Global Trainning Loss    :  0.6511538373906351\n",
      "Win rate = 0.9949584816132859\n",
      "-------------Round number:  96  -------------\n",
      "Average Test Accuracy          :  0.8765479876160991\n",
      "Average Global Trainning Accuracy:  0.884899546338302\n",
      "Average Global Trainning Loss    :  0.650726630853046\n",
      "Win rate = 0.9919928825622776\n",
      "-------------Round number:  97  -------------\n",
      "Average Test Accuracy          :  0.8792569659442725\n",
      "Average Global Trainning Accuracy:  0.8871030460142579\n",
      "Average Global Trainning Loss    :  0.6491517260409916\n",
      "Win rate = 0.9931791221826809\n",
      "-------------Round number:  98  -------------\n",
      "Average Test Accuracy          :  0.878482972136223\n",
      "Average Global Trainning Accuracy:  0.8860661049902787\n",
      "Average Global Trainning Loss    :  0.6463391880670771\n",
      "Win rate = 0.9925860023724793\n",
      "-------------Round number:  99  -------------\n",
      "Average Test Accuracy          :  0.8792569659442725\n",
      "Average Global Trainning Accuracy:  0.8873622812702527\n",
      "Average Global Trainning Loss    :  0.6449901014662994\n",
      "Win rate = 0.9916963226571768\n",
      "-------------Round number:  100  -------------\n",
      "Average Test Accuracy          :  0.8808049535603715\n",
      "Average Global Trainning Accuracy:  0.8885288399222294\n",
      "Average Global Trainning Loss    :  0.6437804424214193\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  101  -------------\n",
      "Average Test Accuracy          :  0.8808049535603715\n",
      "Average Global Trainning Accuracy:  0.8869734283862605\n",
      "Average Global Trainning Loss    :  0.6425940107542125\n",
      "Win rate = 0.9875444839857651\n",
      "-------------Round number:  102  -------------\n",
      "Average Test Accuracy          :  0.8808049535603715\n",
      "Average Global Trainning Accuracy:  0.8880103694102398\n",
      "Average Global Trainning Loss    :  0.6406034181586195\n",
      "Win rate = 0.9884341637010676\n",
      "-------------Round number:  103  -------------\n",
      "Average Test Accuracy          :  0.8777089783281734\n",
      "Average Global Trainning Accuracy:  0.8863253402462735\n",
      "Average Global Trainning Loss    :  0.6389790206780622\n",
      "Win rate = 0.9878410438908659\n",
      "-------------Round number:  104  -------------\n",
      "Average Test Accuracy          :  0.8827399380804953\n",
      "Average Global Trainning Accuracy:  0.8886584575502269\n",
      "Average Global Trainning Loss    :  0.6375311386098509\n",
      "Win rate = 0.9928825622775801\n",
      "-------------Round number:  105  -------------\n",
      "Average Test Accuracy          :  0.8796439628482973\n",
      "Average Global Trainning Accuracy:  0.8899546338302009\n",
      "Average Global Trainning Loss    :  0.6373937363293908\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  106  -------------\n",
      "Average Test Accuracy          :  0.878482972136223\n",
      "Average Global Trainning Accuracy:  0.8873622812702527\n",
      "Average Global Trainning Loss    :  0.6356146584372975\n",
      "Win rate = 0.9884341637010676\n",
      "-------------Round number:  107  -------------\n",
      "Average Test Accuracy          :  0.8808049535603715\n",
      "Average Global Trainning Accuracy:  0.8893065456902138\n",
      "Average Global Trainning Loss    :  0.635332448963059\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  108  -------------\n",
      "Average Test Accuracy          :  0.8792569659442725\n",
      "Average Global Trainning Accuracy:  0.8878807517822424\n",
      "Average Global Trainning Loss    :  0.6350007468203176\n",
      "Win rate = 0.9908066429418743\n",
      "-------------Round number:  109  -------------\n",
      "Average Test Accuracy          :  0.8796439628482973\n",
      "Average Global Trainning Accuracy:  0.8880103694102398\n",
      "Average Global Trainning Loss    :  0.6331913530865197\n",
      "Win rate = 0.9919928825622776\n",
      "-------------Round number:  110  -------------\n",
      "Average Test Accuracy          :  0.8792569659442725\n",
      "Average Global Trainning Accuracy:  0.8885288399222294\n",
      "Average Global Trainning Loss    :  0.6310930411535969\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  111  -------------\n",
      "Average Test Accuracy          :  0.8819659442724458\n",
      "Average Global Trainning Accuracy:  0.8913804277381724\n",
      "Average Global Trainning Loss    :  0.6297321826393389\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  112  -------------\n",
      "Average Test Accuracy          :  0.8819659442724458\n",
      "Average Global Trainning Accuracy:  0.8895657809462086\n",
      "Average Global Trainning Loss    :  0.6295266171824369\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  113  -------------\n",
      "Average Test Accuracy          :  0.8788699690402477\n",
      "Average Global Trainning Accuracy:  0.8851587815942968\n",
      "Average Global Trainning Loss    :  0.6284393607218082\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  114  -------------\n",
      "Average Test Accuracy          :  0.8808049535603715\n",
      "Average Global Trainning Accuracy:  0.8867141931302657\n",
      "Average Global Trainning Loss    :  0.6275058986147116\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  115  -------------\n",
      "Average Test Accuracy          :  0.8792569659442725\n",
      "Average Global Trainning Accuracy:  0.8874918988982502\n",
      "Average Global Trainning Loss    :  0.626403452588302\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  116  -------------\n",
      "Average Test Accuracy          :  0.8804179566563467\n",
      "Average Global Trainning Accuracy:  0.8917692806221647\n",
      "Average Global Trainning Loss    :  0.6261582676806545\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  117  -------------\n",
      "Average Test Accuracy          :  0.8800309597523219\n",
      "Average Global Trainning Accuracy:  0.8903434867141932\n",
      "Average Global Trainning Loss    :  0.62537467595593\n",
      "Win rate = 0.9875444839857651\n",
      "-------------Round number:  118  -------------\n",
      "Average Test Accuracy          :  0.8804179566563467\n",
      "Average Global Trainning Accuracy:  0.8912508101101749\n",
      "Average Global Trainning Loss    :  0.6242534961317239\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  119  -------------\n",
      "Average Test Accuracy          :  0.8761609907120743\n",
      "Average Global Trainning Accuracy:  0.8859364873622813\n",
      "Average Global Trainning Loss    :  0.623714962734932\n",
      "Win rate = 0.9911032028469751\n",
      "-------------Round number:  120  -------------\n",
      "Average Test Accuracy          :  0.8804179566563467\n",
      "Average Global Trainning Accuracy:  0.8885288399222294\n",
      "Average Global Trainning Loss    :  0.6214817168259883\n",
      "Win rate = 0.9925860023724793\n",
      "-------------Round number:  121  -------------\n",
      "Average Test Accuracy          :  0.881578947368421\n",
      "Average Global Trainning Accuracy:  0.8864549578742709\n",
      "Average Global Trainning Loss    :  0.6203068039128321\n",
      "Win rate = 0.9869513641755635\n",
      "-------------Round number:  122  -------------\n",
      "Average Test Accuracy          :  0.8827399380804953\n",
      "Average Global Trainning Accuracy:  0.8885288399222294\n",
      "Average Global Trainning Loss    :  0.6197768779366494\n",
      "Win rate = 0.991399762752076\n",
      "-------------Round number:  123  -------------\n",
      "Average Test Accuracy          :  0.8831269349845201\n",
      "Average Global Trainning Accuracy:  0.8896953985742061\n",
      "Average Global Trainning Loss    :  0.6174915444750486\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  124  -------------\n",
      "Average Test Accuracy          :  0.8823529411764706\n",
      "Average Global Trainning Accuracy:  0.8852883992222942\n",
      "Average Global Trainning Loss    :  0.6158000597456253\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  125  -------------\n",
      "Average Test Accuracy          :  0.8823529411764706\n",
      "Average Global Trainning Accuracy:  0.8860661049902787\n",
      "Average Global Trainning Loss    :  0.613622508911212\n",
      "Win rate = 0.9919928825622776\n",
      "-------------Round number:  126  -------------\n",
      "Average Test Accuracy          :  0.8819659442724458\n",
      "Average Global Trainning Accuracy:  0.8898250162022034\n",
      "Average Global Trainning Loss    :  0.6121254506237849\n",
      "Win rate = 0.9869513641755635\n",
      "-------------Round number:  127  -------------\n",
      "Average Test Accuracy          :  0.8839009287925697\n",
      "Average Global Trainning Accuracy:  0.8902138690861957\n",
      "Average Global Trainning Loss    :  0.6110492698882048\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  128  -------------\n",
      "Average Test Accuracy          :  0.8873839009287926\n",
      "Average Global Trainning Accuracy:  0.8941023979261179\n",
      "Average Global Trainning Loss    :  0.6095142376863253\n",
      "Win rate = 0.9937722419928826\n",
      "-------------Round number:  129  -------------\n",
      "Average Test Accuracy          :  0.8839009287925697\n",
      "Average Global Trainning Accuracy:  0.8911211924821776\n",
      "Average Global Trainning Loss    :  0.6085137516202204\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  130  -------------\n",
      "Average Test Accuracy          :  0.8846749226006192\n",
      "Average Global Trainning Accuracy:  0.8915100453661698\n",
      "Average Global Trainning Loss    :  0.60750750617709\n",
      "Win rate = 0.9878410438908659\n",
      "-------------Round number:  131  -------------\n",
      "Average Test Accuracy          :  0.8827399380804953\n",
      "Average Global Trainning Accuracy:  0.8917692806221647\n",
      "Average Global Trainning Loss    :  0.6068930882412508\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  132  -------------\n",
      "Average Test Accuracy          :  0.8866099071207431\n",
      "Average Global Trainning Accuracy:  0.8935839274141283\n",
      "Average Global Trainning Loss    :  0.6061998744329229\n",
      "Win rate = 0.9881376037959668\n",
      "-------------Round number:  133  -------------\n",
      "Average Test Accuracy          :  0.8866099071207431\n",
      "Average Global Trainning Accuracy:  0.8941023979261179\n",
      "Average Global Trainning Loss    :  0.6046795887678225\n",
      "Win rate = 0.9893238434163701\n",
      "-------------Round number:  134  -------------\n",
      "Average Test Accuracy          :  0.8808049535603715\n",
      "Average Global Trainning Accuracy:  0.888399222294232\n",
      "Average Global Trainning Loss    :  0.6039962481772521\n",
      "Win rate = 0.9925860023724793\n",
      "-------------Round number:  135  -------------\n",
      "Average Test Accuracy          :  0.885061919504644\n",
      "Average Global Trainning Accuracy:  0.8925469863901491\n",
      "Average Global Trainning Loss    :  0.6026996288682761\n",
      "Win rate = 0.9884341637010676\n",
      "-------------Round number:  136  -------------\n",
      "Average Test Accuracy          :  0.8869969040247678\n",
      "Average Global Trainning Accuracy:  0.8939727802981206\n",
      "Average Global Trainning Loss    :  0.6017362939484769\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  137  -------------\n",
      "Average Test Accuracy          :  0.8881578947368421\n",
      "Average Global Trainning Accuracy:  0.8948801036941024\n",
      "Average Global Trainning Loss    :  0.6005801148331174\n",
      "Win rate = 0.9922894424673784\n",
      "-------------Round number:  138  -------------\n",
      "Average Test Accuracy          :  0.8881578947368421\n",
      "Average Global Trainning Accuracy:  0.8946208684381076\n",
      "Average Global Trainning Loss    :  0.5998345603127025\n",
      "Win rate = 0.9931791221826809\n",
      "-------------Round number:  139  -------------\n",
      "Average Test Accuracy          :  0.8866099071207431\n",
      "Average Global Trainning Accuracy:  0.8929358392741413\n",
      "Average Global Trainning Loss    :  0.5983556662143551\n",
      "Win rate = 0.9902135231316725\n",
      "-------------Round number:  140  -------------\n",
      "Average Test Accuracy          :  0.8854489164086687\n",
      "Average Global Trainning Accuracy:  0.8938431626701231\n",
      "Average Global Trainning Loss    :  0.5983999058246922\n",
      "Win rate = 0.9911032028469751\n",
      "-------------Round number:  141  -------------\n",
      "Average Test Accuracy          :  0.885061919504644\n",
      "Average Global Trainning Accuracy:  0.8921581335061568\n",
      "Average Global Trainning Loss    :  0.597328978147278\n",
      "Win rate = 0.9916963226571768\n",
      "-------------Round number:  142  -------------\n",
      "Average Test Accuracy          :  0.8893188854489165\n",
      "Average Global Trainning Accuracy:  0.8952689565780946\n",
      "Average Global Trainning Loss    :  0.5965689935393713\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  143  -------------\n",
      "Average Test Accuracy          :  0.8827399380804953\n",
      "Average Global Trainning Accuracy:  0.8915100453661698\n",
      "Average Global Trainning Loss    :  0.5957463519726183\n",
      "Win rate = 0.9911032028469751\n",
      "-------------Round number:  144  -------------\n",
      "Average Test Accuracy          :  0.8869969040247678\n",
      "Average Global Trainning Accuracy:  0.8943616331821128\n",
      "Average Global Trainning Loss    :  0.5943169505427738\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  145  -------------\n",
      "Average Test Accuracy          :  0.8869969040247678\n",
      "Average Global Trainning Accuracy:  0.895398574206092\n",
      "Average Global Trainning Loss    :  0.5933365906513286\n",
      "Win rate = 0.9893238434163701\n",
      "-------------Round number:  146  -------------\n",
      "Average Test Accuracy          :  0.8842879256965944\n",
      "Average Global Trainning Accuracy:  0.8931950745301361\n",
      "Average Global Trainning Loss    :  0.5918529498136746\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  147  -------------\n",
      "Average Test Accuracy          :  0.8862229102167183\n",
      "Average Global Trainning Accuracy:  0.8931950745301361\n",
      "Average Global Trainning Loss    :  0.5919306064687297\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  148  -------------\n",
      "Average Test Accuracy          :  0.8893188854489165\n",
      "Average Global Trainning Accuracy:  0.8959170447180816\n",
      "Average Global Trainning Loss    :  0.5914738435677253\n",
      "Win rate = 0.9916963226571768\n",
      "-------------Round number:  149  -------------\n",
      "Average Test Accuracy          :  0.8912538699690402\n",
      "Average Global Trainning Accuracy:  0.8956578094620868\n",
      "Average Global Trainning Loss    :  0.5891174355962411\n",
      "Win rate = 0.9908066429418743\n",
      "-------------Round number:  150  -------------\n",
      "Average Test Accuracy          :  0.891640866873065\n",
      "Average Global Trainning Accuracy:  0.8961762799740765\n",
      "Average Global Trainning Loss    :  0.5879969519604666\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  151  -------------\n",
      "Average Test Accuracy          :  0.8943498452012384\n",
      "Average Global Trainning Accuracy:  0.8969539857420609\n",
      "Average Global Trainning Loss    :  0.5868484942077122\n",
      "Win rate = 0.9919928825622776\n",
      "-------------Round number:  152  -------------\n",
      "Average Test Accuracy          :  0.8897058823529411\n",
      "Average Global Trainning Accuracy:  0.894750486066105\n",
      "Average Global Trainning Loss    :  0.586370845653759\n",
      "Win rate = 0.9922894424673784\n",
      "-------------Round number:  153  -------------\n",
      "Average Test Accuracy          :  0.8873839009287926\n",
      "Average Global Trainning Accuracy:  0.8911211924821776\n",
      "Average Global Trainning Loss    :  0.5850927069831497\n",
      "Win rate = 0.991399762752076\n",
      "-------------Round number:  154  -------------\n",
      "Average Test Accuracy          :  0.891640866873065\n",
      "Average Global Trainning Accuracy:  0.8960466623460791\n",
      "Average Global Trainning Loss    :  0.5847020186932923\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  155  -------------\n",
      "Average Test Accuracy          :  0.8931888544891641\n",
      "Average Global Trainning Accuracy:  0.8965651328580687\n",
      "Average Global Trainning Loss    :  0.5839408315780946\n",
      "Win rate = 0.9872479240806643\n",
      "-------------Round number:  156  -------------\n",
      "Average Test Accuracy          :  0.8900928792569659\n",
      "Average Global Trainning Accuracy:  0.8928062216461439\n",
      "Average Global Trainning Loss    :  0.582745855780136\n",
      "Win rate = 0.9872479240806643\n",
      "-------------Round number:  157  -------------\n",
      "Average Test Accuracy          :  0.8889318885448917\n",
      "Average Global Trainning Accuracy:  0.8943616331821128\n",
      "Average Global Trainning Loss    :  0.5810812059502592\n",
      "Win rate = 0.9878410438908659\n",
      "-------------Round number:  158  -------------\n",
      "Average Test Accuracy          :  0.8900928792569659\n",
      "Average Global Trainning Accuracy:  0.8976020738820479\n",
      "Average Global Trainning Loss    :  0.5792256106205443\n",
      "Win rate = 0.9887307236061684\n",
      "-------------Round number:  159  -------------\n",
      "Average Test Accuracy          :  0.8893188854489165\n",
      "Average Global Trainning Accuracy:  0.895398574206092\n",
      "Average Global Trainning Loss    :  0.5787937839233636\n",
      "Win rate = 0.9869513641755635\n",
      "-------------Round number:  160  -------------\n",
      "Average Test Accuracy          :  0.8889318885448917\n",
      "Average Global Trainning Accuracy:  0.8911211924821776\n",
      "Average Global Trainning Loss    :  0.5779534211965327\n",
      "Win rate = 0.9875444839857651\n",
      "-------------Round number:  161  -------------\n",
      "Average Test Accuracy          :  0.8873839009287926\n",
      "Average Global Trainning Accuracy:  0.8903434867141932\n",
      "Average Global Trainning Loss    :  0.577124956962897\n",
      "Win rate = 0.9863582443653618\n",
      "-------------Round number:  162  -------------\n",
      "Average Test Accuracy          :  0.8858359133126935\n",
      "Average Global Trainning Accuracy:  0.8886584575502269\n",
      "Average Global Trainning Loss    :  0.5758423247124109\n",
      "Win rate = 0.9911032028469751\n",
      "-------------Round number:  163  -------------\n",
      "Average Test Accuracy          :  0.8908668730650154\n",
      "Average Global Trainning Accuracy:  0.8924173687621516\n",
      "Average Global Trainning Loss    :  0.574989177434381\n",
      "Win rate = 0.9890272835112692\n",
      "-------------Round number:  164  -------------\n",
      "Average Test Accuracy          :  0.8931888544891641\n",
      "Average Global Trainning Accuracy:  0.8979909267660402\n",
      "Average Global Trainning Loss    :  0.573996349441024\n",
      "Win rate = 0.9875444839857651\n",
      "-------------Round number:  165  -------------\n",
      "Average Test Accuracy          :  0.8904798761609907\n",
      "Average Global Trainning Accuracy:  0.8938431626701231\n",
      "Average Global Trainning Loss    :  0.5733377551847051\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  166  -------------\n",
      "Average Test Accuracy          :  0.8897058823529411\n",
      "Average Global Trainning Accuracy:  0.8921581335061568\n",
      "Average Global Trainning Loss    :  0.5722056262151652\n",
      "Win rate = 0.9905100830367735\n",
      "-------------Round number:  167  -------------\n",
      "Average Test Accuracy          :  0.8939628482972136\n",
      "Average Global Trainning Accuracy:  0.8985093972780298\n",
      "Average Global Trainning Loss    :  0.5711138128848023\n",
      "Win rate = 0.9919928825622776\n",
      "-------------Round number:  168  -------------\n",
      "Average Test Accuracy          :  0.891640866873065\n",
      "Average Global Trainning Accuracy:  0.898250162022035\n",
      "Average Global Trainning Loss    :  0.569553971058814\n",
      "Win rate = 0.9908066429418743\n",
      "-------------Round number:  169  -------------\n",
      "Average Test Accuracy          :  0.8908668730650154\n",
      "Average Global Trainning Accuracy:  0.8950097213220998\n",
      "Average Global Trainning Loss    :  0.569019045183895\n",
      "Win rate = 0.9893238434163701\n",
      "-------------Round number:  170  -------------\n",
      "Average Test Accuracy          :  0.8904798761609907\n",
      "Average Global Trainning Accuracy:  0.895398574206092\n",
      "Average Global Trainning Loss    :  0.5685430421662346\n",
      "Win rate = 0.9908066429418743\n",
      "-------------Round number:  171  -------------\n",
      "Average Test Accuracy          :  0.8900928792569659\n",
      "Average Global Trainning Accuracy:  0.8957874270900843\n",
      "Average Global Trainning Loss    :  0.5681716572828904\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  172  -------------\n",
      "Average Test Accuracy          :  0.8908668730650154\n",
      "Average Global Trainning Accuracy:  0.8955281918340894\n",
      "Average Global Trainning Loss    :  0.5674076221241089\n",
      "Win rate = 0.9899169632265717\n",
      "-------------Round number:  173  -------------\n",
      "Average Test Accuracy          :  0.8943498452012384\n",
      "Average Global Trainning Accuracy:  0.9004536616979909\n",
      "Average Global Trainning Loss    :  0.5671852848549903\n",
      "Win rate = 0.9896204033214709\n",
      "-------------Round number:  174  -------------\n",
      "Average Test Accuracy          :  0.8931888544891641\n",
      "Average Global Trainning Accuracy:  0.8986390149060273\n",
      "Average Global Trainning Loss    :  0.5662511265594621\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(\n\u001b[0;32m      2\u001b[0m         api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq24VqIVkFNEOugLA3T0YFFFvE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m         project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msophia\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m         workspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabdulmomen96\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     )\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msophiaOTA_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m experiment\u001b[38;5;241m.\u001b[39mend()    \n",
      "Cell \u001b[1;32mIn[19], line 63\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters, local_epochs, optimizer, numedges, times, commet, gpu)\u001b[0m\n\u001b[0;32m     60\u001b[0m     experiment\u001b[38;5;241m.\u001b[39mset_name(dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m algorithm \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(learning_rate) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(alpha) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mal_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(eta) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(rho) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m  \u001b[38;5;28mstr\u001b[39m(num_glob_iters) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mge_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(local_epochs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mle_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(numedges) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m server \u001b[38;5;241m=\u001b[39m Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n\u001b[1;32m---> 63\u001b[0m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m server\u001b[38;5;241m.\u001b[39mtest()\n",
      "File \u001b[1;32m~\\Desktop\\Git_Projects\\DONE\\algorithms\\server\\server.py:370\u001b[0m, in \u001b[0;36mServer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# self.evaluate()  # still evaluate on the global model\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medges:\n\u001b[1;32m--> 370\u001b[0m     \u001b[43medge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_grads_sophia_OTA()\n\u001b[0;32m    374\u001b[0m hess \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_hessians_sophia_OTA()\n",
      "File \u001b[1;32m~\\Desktop\\Git_Projects\\DONE\\algorithms\\edges\\edgeSophia.py:36\u001b[0m, in \u001b[0;36medgeSophia.train\u001b[1;34m(self, epochs, glob_iter)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs, glob_iter):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Only update once time\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainloaderfull):\n\u001b[0;32m     37\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m#self.optimizer.zero_grad()\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [default_collate(samples) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    172\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    139\u001b[0m         storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    140\u001b[0m         out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr_\u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndarray\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_type\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemmap\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophiaOTA_params)\n",
    "experiment.end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 11220132,  Total Parameters: 11220132\n"
     ]
    }
   ],
   "source": [
    "model = resnet18()\n",
    "total_params = sum(\n",
    "\tparam.numel() for param in model.parameters()\n",
    ")\n",
    "\n",
    "\n",
    "trainable_params = sum(\n",
    "\tparam.numel() for param in model.parameters() if param.requires_grad\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params},  Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
