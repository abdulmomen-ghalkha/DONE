{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from comet_ml import Experiment\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "def main(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters,\n",
    "         local_epochs, optimizer, numedges, times, commet, gpu):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "    for i in range(times):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            if(dataset == \"human_activity\"):\n",
    "                model = Mclr_CrossEntropy(561,6).to(device), model\n",
    "                print(\"Hey\")\n",
    "            else:\n",
    "                model = Mclr_Logistic().to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(40,1).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(300).to(device), model\n",
    "        \n",
    "        if model == \"MLP\" and dataset == \"a9a\":\n",
    "            model = DNN( input_dim = 123, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"human_activity\":\n",
    "            model = DNN( input_dim = 561, mid_dim = 561, output_dim = 6).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"w8a\":\n",
    "            model = DNN( input_dim = 300, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"Mnist\":\n",
    "            model = DNN().to(device), model\n",
    "        if model == 'CNN':\n",
    "            model = Net().to(device), model\n",
    "        # select algorithm\n",
    "        if(commet):\n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\")\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sophiaOTA_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia_OTA\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 1280,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "sophia_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.003,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 41,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "GD_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"GD\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "FedProx = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"FedProx\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.01,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 10,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "'''\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **FedProx)\n",
    "experiment.end()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophiaOTA_params)\n",
    "experiment.end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophia_params)\n",
    "experiment.end()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Human Activity\n",
    "Sophia\n",
    "-------------Round number:  40  -------------\n",
    "Average Test Accuracy          :  0.8502321981424149\n",
    "Average Global Trainning Accuracy:  0.8495139338950097\n",
    "Average Global Trainning Loss    :  1.0823885845349968\n",
    "Clipping rate = 0.576000928907216\n",
    "\n",
    "\"learning_rate\": 0.0001,\n",
    "    \"alpha\": (0.9, 0.95),\n",
    "    \"eta\": 1.0 * 50,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 20,\n",
    "\n",
    "\n",
    "GD\n",
    "-------------Round number:  40  -------------\n",
    "Average Global Accuracy          :  0.20936532507739938\n",
    "Average Global Trainning Accuracy:  0.20764744005184704\n",
    "Average Global Trainning Loss    :  1.7220779072828905\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "MNIST MLP\n",
    "Sophia \n",
    "-------------Round number:  39  -------------\n",
    "Average Global Accuracy          :  0.8934867045115028\n",
    "Average Global Trainning Accuracy:  0.8913894080996885\n",
    "Average Global Trainning Loss    :  0.3557637022975078\n",
    "\n",
    "GD\n",
    "-------------Round number:  40  -------------\n",
    "Average Global Accuracy          :  0.5679713175978488\n",
    "Average Global Trainning Accuracy:  0.5671775700934579\n",
    "Average Global Trainning Loss    :  1.9333872663551401\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(\n",
    "\tparam.numel() for param in model.parameters()\n",
    ")\n",
    "\n",
    "\n",
    "trainable_params = sum(\n",
    "\tparam.numel() for param in model.parameters() if param.requires_grad\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params},  Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = [param.numel() for param in model.parameters() if param.requires_grad]\n",
    "trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "total_params = 0\n",
    "for layer in model.children():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        total_params += layer.state_dict()['weight'].numel() + layer.state_dict()['bias'].numel()\n",
    "        print(layer.state_dict()['weight'].numel())\n",
    "        print(layer.state_dict()['bias'].numel())\n",
    "        print(layer.state_dict()['weight'].shape)\n",
    "        print(layer.state_dict()['bias'].shape)\n",
    "        \n",
    "print(f\"Total Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import get_training_data_value, simple_read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rs_train_acc_sop, rs_train_loss_sop, rs_glob_acc_sop = simple_read_data(\"Mnist_Sophia_0.01_(0.9, 0.95)_20.0_0.1_32u_64b_1_0_25db\")\n",
    "rs_train_acc_done, rs_train_loss_done, rs_glob_acc_done = simple_read_data(\"Mnist_DONE_1_0.003_1.0_0.001_32u_0b_40_0\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_gd, rs_train_loss_gd, rs_glob_acc_gd = simple_read_data(\"Mnist_GD_0.01_0.03_20.0_0.001_30u_64b_1_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "print(rs_glob_acc_sop[:80].shape)\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop[:80], label='FedSophia-25dB', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_glob_acc_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "#plt.plot(x, rs_glob_acc_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop[:80], label='FedSophia-25dB', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_train_loss_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "#plt.plot(x, rs_train_loss_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rs_train_acc_sop, rs_train_loss_sop, rs_glob_acc_sop = simple_read_data(\"Fashion_Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.1_20u_64b_1_0\")\n",
    "#rs_train_acc_done, rs_train_loss_done, rs_glob_acc_done = simple_read_data(\"Mnist_DONE_1_0.003_1.0_0.001_32u_0b_40_0\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_gd, rs_train_loss_gd, rs_glob_acc_gd = simple_read_data(\"Fashion_Mnist_GD_0.01_0.03_20.0_0.001_20u_64b_1_0\")\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "print(rs_glob_acc_sop[:80].shape)\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_glob_acc_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_glob_acc_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_train_loss_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_train_loss_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rs_train_acc_sop, rs_train_loss_sop, rs_glob_acc_sop = simple_read_data(\"Mnist_Sophia_0.005_(0.9, 0.95)_20.0_0.1_32u_64b_1_0_CNN_IID\")\n",
    "#rs_train_acc_done, rs_train_loss_done, rs_glob_acc_done = simple_read_data(\"Mnist_DONE_1_0.003_1.0_0.001_32u_0b_40_0\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_gd, rs_train_loss_gd, rs_glob_acc_gd = simple_read_data(\"Mnist_GD_0.01_0.03_20.0_0.001_30u_64b_1_0_CNN_IID\")\n",
    "\n",
    "b = rs_train_loss_gd\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "print(rs_glob_acc_sop[:80].shape)\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_glob_acc_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_glob_acc_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_train_loss_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_train_loss_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rs_train_acc_sop, rs_train_loss_sop, rs_glob_acc_sop = simple_read_data(\"Mnist_Sophia_0.005_(0.9, 0.95)_20.0_0.1_32u_64b_1_0_CNN_NIID\")\n",
    "#rs_train_acc_done, rs_train_loss_done, rs_glob_acc_done = simple_read_data(\"Mnist_DONE_1_0.003_1.0_0.001_32u_0b_40_0\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_gd, rs_train_loss_gd, rs_glob_acc_gd = simple_read_data(\"Mnist_GD_0.01_0.03_20.0_0.001_30u_64b_1_0_CNN_NIID\")\n",
    "\n",
    "a = rs_train_loss_gd\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "print(rs_glob_acc_sop[:80].shape)\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_glob_acc_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_glob_acc_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop[:80], label='FedSophia', linestyle='--', marker='o', markevery=4)\n",
    "#plt.plot(x_DONE, rs_train_loss_done, label='DONE', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x, rs_train_loss_gd, label='FedAVG', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sopgia with SNRs\n",
    "\n",
    "from utils.plot_utils import get_training_data_value, simple_read_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rs_train_acc_sop25, rs_train_loss_sop25, rs_glob_acc_sop25 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_25dB\")\n",
    "rs_train_acc_sop30, rs_train_loss_sop30, rs_glob_acc_sop30 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_30dB\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_sop35, rs_train_loss_sop35, rs_glob_acc_sop35 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_35dB\")\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop25[:80], label='SNR=25dB', linestyle='--', marker='o', markevery=4)\n",
    "plt.plot(x[:80], rs_glob_acc_sop30[:80], label='SNR=30dB', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x[:80], rs_glob_acc_sop35[:80], label='SNR=35dB', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop25[:80], label='SNR=25dB', linestyle='--', marker='o', markevery=4)\n",
    "plt.plot(x[:80], rs_train_loss_sop30[:80], label='SNR=30dB', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x[:80], rs_train_loss_sop35[:80], label='SNR=35dB', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sopgia with SNRs\n",
    "\n",
    "from utils.plot_utils import get_training_data_value, simple_read_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rs_train_acc_sop25, rs_train_loss_sop25, rs_glob_acc_sop25 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_fix_5dB\")\n",
    "rs_train_acc_sop30, rs_train_loss_sop30, rs_glob_acc_sop30 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_fix_25dB\")\n",
    "#(\"Mnist_DONE_1_0.008_1.0_0.001_32u_0b_40_0\")\n",
    "rs_train_acc_sop35, rs_train_loss_sop35, rs_glob_acc_sop35 = simple_read_data(\"Mnist_Sophia_0.001_(0.9, 0.95)_20.0_0.001_32u_64b_1_0_fix_35dB\")\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(0, 82)\n",
    "x_DONE = np.arange(0, 2 * 41, 2)\n",
    "xlim = 41\n",
    "\n",
    "# Subplot 1: Training Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x[:80], rs_glob_acc_sop25[:80], label='SNR=5dB', linestyle='--', marker='o', markevery=4)\n",
    "plt.plot(x[:80], rs_glob_acc_sop30[:80], label='SNR=25dB', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x[:80], rs_glob_acc_sop35[:80], label='SNR=35dB', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Training Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x[:80], rs_train_loss_sop25[:80], label='SNR=5dB', linestyle='--', marker='o', markevery=4)\n",
    "plt.plot(x[:80], rs_train_loss_sop30[:80], label='SNR=25dB', linestyle='--', marker='s', markevery=4)\n",
    "plt.plot(x[:80], rs_train_loss_sop35[:80], label='SNR=35dB', linestyle='--', marker='D', markevery=4)\n",
    "\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Mnist_MLP_.eps', format='eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a - b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.027\n",
    "\n",
    "np.mean(np.equal(np.ones((20, 1), np.float32), 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN( input_dim = 123, output_dim = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        #print(grads[i].sign())\n",
    "#print(f\"Clipping rate = {1 - (winrate / param_count)}\")\n",
    "x = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mnist MLP settings\n",
    "sophia_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20,\n",
    "    \"L\": 0.1,\n",
    "    \"rho\": 20,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.003,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 41,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "GD_params = {\n",
    "    \"dataset\": \"Mnist\",\n",
    "    \"algorithm\": \"GD\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 2, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(18432, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(2, 1)(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(2, 1)(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x#F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {param_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN28(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN28, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Adjusted input size based on the dimensions after pooling\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.ReLU()(self.conv1(x)))\n",
    "        x = self.pool(nn.ReLU()(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = DNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total number of parameters: {param_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "421642 / 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from comet_ml import Experiment\n",
    "#\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "def main(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters,\n",
    "         local_epochs, optimizer, numedges, times, commet, gpu):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "    for i in range(times):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            if(dataset == \"human_activity\"):\n",
    "                model = Mclr_CrossEntropy(561,6).to(device), model\n",
    "                print(\"Hey\")\n",
    "            else:\n",
    "                model = Mclr_Logistic().to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(40,1).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(300).to(device), model\n",
    "        \n",
    "        if model == \"MLP\" and dataset == \"a9a\":\n",
    "            model = DNN( input_dim = 123, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"human_activity\":\n",
    "            model = DNN( input_dim = 561, mid_dim = 561, output_dim = 6).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"w8a\":\n",
    "            model = DNN( input_dim = 300, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"Mnist\":\n",
    "            model = DNN().to(device), model\n",
    "        if model == 'CNN':\n",
    "            model = Net().to(device), model\n",
    "        # select algorithm\n",
    "        if model == \"Resnet\":\n",
    "            model = resnet18().to(device), model\n",
    "        if(commet):\n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\")\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sophiaOTA_params = {\n",
    "    \"dataset\": \"Cifar10\",\n",
    "    \"algorithm\": \"Sophia_OTA\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 1280,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 20,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "sophia_params = {\n",
    "    \"dataset\": \"Cifar10\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Cifar10\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.003,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 41,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "GD_params = {\n",
    "    \"dataset\": \"Cifar10\",\n",
    "    \"algorithm\": \"GD\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "FedProx = {\n",
    "    \"dataset\": \"Cifar10\",\n",
    "    \"algorithm\": \"FedProx\",\n",
    "    \"model\": \"CNN\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.008,\n",
    "    \"alpha\": 0.001,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 300,\n",
    "    \"local_epochs\": 10,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "'''\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **FedProx)\n",
    "experiment.end()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophia_params)\n",
    "experiment.end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophiaOTA_params)\n",
    "experiment.end()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **GD_params)\n",
    "experiment.end()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet18 and Cifar100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from comet_ml import Experiment\n",
    "#\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "def main(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters,\n",
    "         local_epochs, optimizer, numedges, times, commet, gpu):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "    for i in range(times):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            if(dataset == \"human_activity\"):\n",
    "                model = Mclr_CrossEntropy(561,6).to(device), model\n",
    "                print(\"Hey\")\n",
    "            else:\n",
    "                model = Mclr_Logistic().to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(40,1).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(300).to(device), model\n",
    "        \n",
    "        if model == \"MLP\" and dataset == \"a9a\":\n",
    "            model = DNN( input_dim = 123, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"human_activity\":\n",
    "            model = DNN( input_dim = 561, mid_dim = 561, output_dim = 6).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"w8a\":\n",
    "            model = DNN( input_dim = 300, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"Mnist\":\n",
    "            model = DNN().to(device), model\n",
    "        if model == 'CNN':\n",
    "            model = Net().to(device), model\n",
    "        # select algorithm\n",
    "        if model == \"Resnet\":\n",
    "            model = resnet18().to(device), model\n",
    "        if(commet):\n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\")\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda:{}\".format(0) if torch.cuda.is_available() and 0 != -1 else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sophiaOTA_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"Sophia_OTA\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 1280,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 20,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "sophia_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 50.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 5,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.003,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 41,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "GD_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"GD\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 10,\n",
    "    \"times\": 5,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "FedProx = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"FedProx\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.008,\n",
    "    \"alpha\": 0.005,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 10,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 10,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **FedProx)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **GD_params)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophia_params)\n",
    "experiment.end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **sophiaOTA_params)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sent140 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from comet_ml import Experiment\n",
    "#\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "import importlib\n",
    "import random\n",
    "import os\n",
    "from algorithms.server.server import Server\n",
    "from algorithms.trainmodel.models import *\n",
    "from utils.plot_utils import *\n",
    "import torch\n",
    "\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "\n",
    "\n",
    "def main(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters,\n",
    "         local_epochs, optimizer, numedges, times, commet, gpu):\n",
    "\n",
    "    device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() and gpu != -1 else \"cpu\")\n",
    "\n",
    "    for i in range(times):\n",
    "        print(\"---------------Running time:------------\",i)\n",
    "\n",
    "        # Generate model\n",
    "        if(model == \"mclr\"):\n",
    "            if(dataset == \"human_activity\"):\n",
    "                model = Mclr_CrossEntropy(561,6).to(device), model\n",
    "                print(\"Hey\")\n",
    "            else:\n",
    "                model = Mclr_Logistic().to(device), model\n",
    "\n",
    "        if(model == \"linear_regression\"):\n",
    "            model = Linear_Regression(40,1).to(device), model\n",
    "\n",
    "        if model == \"logistic_regression\":\n",
    "            model = Logistic_Regression(300).to(device), model\n",
    "        \n",
    "        if model == \"MLP\" and dataset == \"a9a\":\n",
    "            model = DNN( input_dim = 123, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"human_activity\":\n",
    "            model = DNN( input_dim = 561, mid_dim = 561, output_dim = 6).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"w8a\":\n",
    "            model = DNN( input_dim = 300, output_dim = 2).to(device), model\n",
    "        if model == \"MLP\" and dataset == \"Mnist\":\n",
    "            model = DNN().to(device), model\n",
    "        if model == 'CNN':\n",
    "            model = Net().to(device), model\n",
    "        # select algorithm\n",
    "        if model == \"Resnet\":\n",
    "            model = resnet18().to(device), model\n",
    "        if model == \"LSTM\" and dataset == \"Sent140\":\n",
    "            model = Sent140(num_classes=2, n_hidden=100, num_embeddings=101, embedding_dim=100, max_seq_len=50, dropout_rate=0.1).to(device), model \n",
    "        if(commet):\n",
    "            experiment.set_name(dataset + \"_\" + algorithm + \"_\" + model[1] + \"_\" + str(batch_size) + \"b_\" + str(learning_rate) + \"lr_\" + str(alpha) + \"al_\" + str(eta) + \"eta_\" + str(L) + \"L_\" + str(rho) + \"p_\" +  str(num_glob_iters) + \"ge_\"+ str(local_epochs) + \"le_\"+ str(numedges) +\"u\")\n",
    "        server = Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n",
    "        \n",
    "        server.train()\n",
    "        server.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNewton_params = {\\n    \"dataset\": \"Cifar100\",\\n    \"algorithm\": \"Newton\",\\n    \"model\": \"MLP\",\\n    \"batch_size\": 0,\\n    \"learning_rate\": 1,\\n    \"alpha\": 0.03,\\n    \"eta\": 1.0,\\n    \"L\": 1e-3,\\n    \"rho\": 0.01,\\n    \"num_glob_iters\": 500,\\n    \"local_epochs\": 40,\\n    \"optimizer\": \"Newton\",\\n    \"numedges\": 30,\\n    \"times\": 1,\\n    \"commet\": True,\\n    \"gpu\": 0\\n}\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sophiaOTA_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"Sophia_OTA\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 20.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 1280,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 20,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "sophia_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"Sophia\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"alpha\": (0.90, 0.95),\n",
    "    \"eta\": 1.0 * 50.,\n",
    "    \"L\": 0.001,\n",
    "    \"rho\": 200,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"Sophia\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 5,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "DONE_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"DONE\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.003,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 41,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 32,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "GD_params = {\n",
    "    \"dataset\": \"Sent140\",\n",
    "    \"algorithm\": \"GD\",\n",
    "    \"model\": \"LSTM\",\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 1,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 20,\n",
    "    \"times\": 5,\n",
    "    \"commet\": False,\n",
    "    \"gpu\":0\n",
    "}\n",
    "\n",
    "FedProx = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"FedProx\",\n",
    "    \"model\": \"Resnet\",\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 0.008,\n",
    "    \"alpha\": 0.005,\n",
    "    \"eta\": 20.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 2 * 41,\n",
    "    \"local_epochs\": 10,\n",
    "    \"optimizer\": \"DONE\",\n",
    "    \"numedges\": 10,\n",
    "    \"times\": 1,\n",
    "    \"commet\": False,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "Newton_params = {\n",
    "    \"dataset\": \"Cifar100\",\n",
    "    \"algorithm\": \"Newton\",\n",
    "    \"model\": \"MLP\",\n",
    "    \"batch_size\": 0,\n",
    "    \"learning_rate\": 1,\n",
    "    \"alpha\": 0.03,\n",
    "    \"eta\": 1.0,\n",
    "    \"L\": 1e-3,\n",
    "    \"rho\": 0.01,\n",
    "    \"num_glob_iters\": 500,\n",
    "    \"local_epochs\": 40,\n",
    "    \"optimizer\": \"Newton\",\n",
    "    \"numedges\": 30,\n",
    "    \"times\": 1,\n",
    "    \"commet\": True,\n",
    "    \"gpu\": 0\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/abdulmomen96/sophia/a39ac6bf304e49eaae36d95e4405eecd\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------Running time:------------ 0\n",
      "Number of edges / total edges: 20  /  32\n",
      "-------------Round number:  0  -------------\n",
      "Average Test Accuracy          :  0.5019718716672592\n",
      "Average Global Trainning Accuracy:  0.5023482436544652\n",
      "Average Global Trainning Loss    :  0.693322642105078\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m experiment \u001b[38;5;241m=\u001b[39m Experiment(\n\u001b[0;32m      2\u001b[0m         api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq24VqIVkFNEOugLA3T0YFFFvE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m         project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msophia\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m         workspace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabdulmomen96\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     )\n\u001b[1;32m----> 6\u001b[0m main(experiment, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mGD_params)\n\u001b[0;32m      7\u001b[0m experiment\u001b[38;5;241m.\u001b[39mend()\n",
      "Cell \u001b[1;32mIn[1], line 65\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(experiment, dataset, algorithm, model, batch_size, learning_rate, alpha, eta, L, rho, num_glob_iters, local_epochs, optimizer, numedges, times, commet, gpu)\u001b[0m\n\u001b[0;32m     62\u001b[0m     experiment\u001b[38;5;241m.\u001b[39mset_name(dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m algorithm \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(batch_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(learning_rate) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(alpha) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mal_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(eta) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meta_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(L) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(rho) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m  \u001b[38;5;28mstr\u001b[39m(num_glob_iters) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mge_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(local_epochs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mle_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(numedges) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m server \u001b[38;5;241m=\u001b[39m Server(experiment, device, dataset, algorithm, model, batch_size, learning_rate, alpha, eta,  L, num_glob_iters, local_epochs, optimizer, numedges, i)\n\u001b[1;32m---> 65\u001b[0m server\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     66\u001b[0m server\u001b[38;5;241m.\u001b[39mtest()\n",
      "File \u001b[1;32m~\\Desktop\\Git_Projects\\DONE\\algorithms\\server\\server.py:203\u001b[0m, in \u001b[0;36mServer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_edges(glob_iter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_edges)\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_edges:\n\u001b[1;32m--> 203\u001b[0m             edge\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_epochs, glob_iter)\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_parameters()\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDONE\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# Second Order method\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Git_Projects\\DONE\\algorithms\\edges\\edgeGD.py:25\u001b[0m, in \u001b[0;36medgeGD.train\u001b[1;34m(self, epochs, glob_iter)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Only update once time\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_next_train_batch()):\n\u001b[0;32m     26\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint32), y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "        api_key=\"q24VqIVkFNEOugLA3T0YFFFvE\",\n",
    "        project_name=\"sophia\",\n",
    "        workspace=\"abdulmomen96\",\n",
    "    )\n",
    "main(experiment, **GD_params)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
